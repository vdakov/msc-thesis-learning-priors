{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ac15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Securely input your token\n",
    "token = getpass('Paste your GitLab token: ')\n",
    "\n",
    "# Set the repository URL\n",
    "# repo_url = \"https://oauth2:\" + token + \"@gitlab.ewi.tudelft.nl/dsait5000/tom-viering/msc-thesis-vasko.git\"\n",
    "username = \"vdakov\" \n",
    "\n",
    "# Syntax: https://<username>:<token>@<domain>/...\n",
    "repo_url = f\"https://{username}:{token}@gitlab.ewi.tudelft.nl/dsait5000/tom-viering/msc-thesis-vasko.git\"\n",
    "\n",
    "!git clone {repo_url}\n",
    "\n",
    "# Clone the repository\n",
    "# !git clone {repo_url}\n",
    "# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob\n",
    "# Verify clone\n",
    "os.chdir(\"msc-thesis-vasko\")\n",
    "%pip install -r requirements.txt\n",
    "os.chdir(\"notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c539e1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: /home/vdakov/Desktop/thesis/msc-thesis-vasko\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(os.path.join(parent_dir, 'src'))\n",
    "print(f\"Added to sys.path: {parent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f689e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Distribution\n",
    "\n",
    "class DistributionSampler:\n",
    "    def __init__(self, distribution: Distribution):\n",
    "        \"\"\"\n",
    "        :param distribution: An instantiated torch.distributions object \n",
    "                             (e.g., Uniform(0.1, 3.0), Gamma(2.0, 2.0))\n",
    "        \"\"\"\n",
    "        self.distribution = distribution\n",
    "\n",
    "    def sample(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the distribution with the given batch size.\n",
    "        Returns a tensor of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # PyTorch distributions expect a tuple/torch.Size for sampling\n",
    "        return self.distribution.sample(torch.Size([batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f622c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Process with HyperPrior\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import models.encoders as encoders\n",
    "from training_util import get_uniform_single_eval_pos_sampler, get_weighted_single_eval_pos_sampler, get_cosine_schedule_with_warmup\n",
    "import train\n",
    "from criterion.bar_distribution import BarDistribution, get_bucket_limits\n",
    "from models import positional_encodings\n",
    "from prior_generation import gp_prior, gp_lengthscale_prior\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "epochs = 2000\n",
    "batch_size =  256\n",
    "warmup_epochs = 25\n",
    "steps_per_epoch = 10\n",
    "lr = 0.0001\n",
    "sequence_length = 10\n",
    "emsize = 512\n",
    "fuse_x_y = False\n",
    "nlayers = 6\n",
    "nhead = 4\n",
    "nhid = 1024\n",
    "dropout = 0.2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_buckets = 100\n",
    "min_y = -10\n",
    "max_y = 10\n",
    "num_features = 1\n",
    "num_outputs = 100\n",
    "my_prior_dist = dist.Uniform(low=0.1, high=1)\n",
    "\n",
    "sampler = DistributionSampler(my_prior_dist)\n",
    "# prior_hyperparameters = {'num_features': num_features, 'num_outputs': num_outputs, 'device': device, 'kernel': \"rbf\", 'length_scale': 0.5}\n",
    "prior_hyperparameters = {'num_features': num_features, 'num_outputs': num_outputs, 'device': device, 'kernel': \"rbf\", 'length_scale': 0.5, \"length_scale_sampling\": sampler}\n",
    "input_normalization = True\n",
    "aggregate_k_gradients=1\n",
    "encoder_type = 'linear'  # 'linear' or 'mlp'\n",
    "y_encoder_type = 'linear'\n",
    "pos_encoder_type = 'none'  # 'sinus', 'learned', 'none'\n",
    "scheduler = get_cosine_schedule_with_warmup\n",
    "prior_prediction = True\n",
    "\n",
    "def get_encoder_generator(encoder):\n",
    "        if encoder == 'linear':\n",
    "            encoder_generator = encoders.LinearEncoder\n",
    "        elif encoder == 'mlp':\n",
    "            encoder_generator = encoders.MLPEncoder\n",
    "        else:\n",
    "            raise NotImplementedError(f'A {encoder} encoder is not valid.')\n",
    "        return encoder_generator\n",
    "\n",
    "encoder_generator = get_encoder_generator(encoder_type)\n",
    "y_encoder_generator = get_encoder_generator(y_encoder_type)\n",
    "\n",
    "if pos_encoder_type== 'sinus':\n",
    "    pos_encoder_generator = positional_encodings.PositionalEncoding\n",
    "elif pos_encoder_type == 'learned':\n",
    "    pos_encoder_generator = positional_encodings.LearnedPositionalEncoding\n",
    "else:\n",
    "    pos_encoder_generator = positional_encodings.NoPositionalEncoding\n",
    "    \n",
    "permutation_invariant_max_eval_pos = sequence_length - 1\n",
    "permutation_invariant_sampling = 'uniform'\n",
    "\n",
    "if permutation_invariant_max_eval_pos is not None:\n",
    "    if permutation_invariant_sampling == 'weighted':\n",
    "        get_sampler = get_weighted_single_eval_pos_sampler\n",
    "    elif permutation_invariant_sampling == 'uniform':\n",
    "        get_sampler = get_uniform_single_eval_pos_sampler\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    \n",
    "context_delimiter_generator = get_sampler(permutation_invariant_max_eval_pos)\n",
    "\n",
    "transformer_configuration = (emsize, nhead, nhid, nlayers, dropout, num_features, num_outputs, input_normalization, y_encoder_generator, sequence_length, fuse_x_y, prior_prediction) \n",
    "training_configuration = (epochs, steps_per_epoch, batch_size, sequence_length, lr, warmup_epochs, aggregate_k_gradients, scheduler, prior_prediction)\n",
    "generators = (encoder_generator, y_encoder_generator, pos_encoder_generator)\n",
    "# prior = gp_prior.GaussianProcessPriorGenerator()\n",
    "prior = gp_lengthscale_prior.GaussianProcessHyperPriorGenerator()\n",
    "print(prior.name)\n",
    "criterion = BarDistribution(borders=get_bucket_limits(num_buckets, full_range=(min_y, max_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb480b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m hyperparameters = { \u001b[33m'\u001b[39m\u001b[33mkernel\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mrbf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlength_scale\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.1\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mprior\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber_of_datasets\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_points_per_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features_per_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/thesis/msc-thesis-vasko/src/prior_generation/prior_generator.py:30\u001b[39m, in \u001b[36mPriorGenerator.visualize_datasets\u001b[39m\u001b[34m(self, number_of_datasets, num_points_per_dataset, num_features_per_dataset, device, **hyperparameter_configuration_kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvisualize_datasets\u001b[39m(\u001b[38;5;28mself\u001b[39m, number_of_datasets, num_points_per_dataset, num_features_per_dataset, device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m, **hyperparameter_configuration_kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     datasets = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_datasets_from_prior\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber_of_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_points_per_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features_per_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhyperparameter_configuration_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     x, y, y_noisy = datasets \n\u001b[32m     32\u001b[39m     x, y, y_noisy = x.detach().numpy() , y.detach().numpy() , y_noisy.detach().numpy() \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/thesis/msc-thesis-vasko/src/prior_generation/prior_generator.py:25\u001b[39m, in \u001b[36mPriorGenerator.get_datasets_from_prior\u001b[39m\u001b[34m(self, number_of_datasets, num_points_per_dataset, num_features_per_dataset, device, **hyperparameter_configuration_kwargs)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_datasets_from_prior\u001b[39m(\u001b[38;5;28mself\u001b[39m, number_of_datasets, num_points_per_dataset, num_features_per_dataset, device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m, **hyperparameter_configuration_kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     x, y, y_noisy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber_of_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_points_per_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features_per_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhyperparameter_configuration_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x, y, y_noisy\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/thesis/msc-thesis-vasko/src/prior_generation/gp_lengthscale_prior.py:56\u001b[39m, in \u001b[36mGaussianProcessHyperPriorGenerator.get_batch\u001b[39m\u001b[34m(self, batch_size, seq_len, num_features, device, **hyperparameter_configuration_kwargs)\u001b[39m\n\u001b[32m     53\u001b[39m output_scale = hyperparameter_configuration_kwargs.get(\u001b[33m'\u001b[39m\u001b[33moutput_scale\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m) \u001b[38;5;66;03m#type ignore \u001b[39;00m\n\u001b[32m     54\u001b[39m noise_std = hyperparameter_configuration_kwargs.get(\u001b[33m'\u001b[39m\u001b[33mnoise_std\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.1\u001b[39m) \u001b[38;5;66;03m#type ignore \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m length_scale =  \u001b[43mlength_scale_sampling\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m(batch_size).to(device)\n\u001b[32m     57\u001b[39m length_scale = length_scale.view(batch_size, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     59\u001b[39m kernel = \u001b[38;5;28mself\u001b[39m._get_kernel(kernel_name, batch_size, **hyperparameter_configuration_kwargs) \u001b[38;5;66;03m#type ignore \u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "hyperparameters = { 'kernel': \"rbf\", 'length_scale': 0.1}\n",
    "prior.visualize_datasets(number_of_datasets=5, num_points_per_dataset=200, num_features_per_dataset=1, device='cpu', **hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d125b87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdakov/.conda/envs/thesis/lib/python3.14/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/vdakov/.conda/envs/thesis/lib/python3.14/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu...\n",
      "Using cpu:0 device\n",
      "Dataset.__dict__ {'num_steps': 10, 'fuse_x_y': False, 'prior_prediction': True, 'get_batch_kwargs': {'batch_size': 256, 'seq_len': 10, 'num_features': 1, 'num_outputs': 100, 'device': 'cpu', 'kernel': 'rbf', 'length_scale': 0.5, 'length_scale_sampling': <__main__.DistributionSampler object at 0x7a773d636ba0>}, 'num_features': 1, 'num_outputs': 100}\n",
      "DataLoader.__dict__ {'num_steps': 10, 'fuse_x_y': False, 'get_batch_kwargs': {'batch_size': 256, 'seq_len': 10, 'prior_prediction': True, 'num_features': 1, 'num_outputs': 100, 'device': 'cpu', 'kernel': 'rbf', 'length_scale': 0.5, 'length_scale_sampling': <__main__.DistributionSampler object at 0x7a773d636ba0>}, 'PriorDataset': <class 'prior_generation.prior_dataloader.get_dataloader.<locals>.PriorDataset'>, 'num_features': 1, 'num_outputs': 100, 'validation_set': ((tensor([[[0.7781],\n",
      "         [0.7080],\n",
      "         [0.2171],\n",
      "         ...,\n",
      "         [0.7242],\n",
      "         [0.5296],\n",
      "         [0.9530]],\n",
      "\n",
      "        [[0.9831],\n",
      "         [0.7349],\n",
      "         [0.9699],\n",
      "         ...,\n",
      "         [0.2185],\n",
      "         [0.3587],\n",
      "         [0.6072]],\n",
      "\n",
      "        [[0.7279],\n",
      "         [0.3990],\n",
      "         [0.7878],\n",
      "         ...,\n",
      "         [0.9468],\n",
      "         [0.3408],\n",
      "         [0.5999]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.8310],\n",
      "         [0.5396],\n",
      "         [0.9839],\n",
      "         ...,\n",
      "         [0.2030],\n",
      "         [0.2004],\n",
      "         [0.2747]],\n",
      "\n",
      "        [[0.3396],\n",
      "         [0.8319],\n",
      "         [0.2557],\n",
      "         ...,\n",
      "         [0.1811],\n",
      "         [0.6973],\n",
      "         [0.5771]],\n",
      "\n",
      "        [[0.1564],\n",
      "         [0.3024],\n",
      "         [0.3650],\n",
      "         ...,\n",
      "         [0.2726],\n",
      "         [0.3589],\n",
      "         [0.5013]]]), tensor([[-0.5132, -1.8877,  0.7946,  ...,  0.2082, -0.8849,  0.4074],\n",
      "        [-0.5090, -1.9425,  0.2471,  ..., -0.6627, -1.3643,  0.2750],\n",
      "        [-0.5333, -0.4339,  0.0248,  ...,  0.6633, -1.4107,  0.2690],\n",
      "        ...,\n",
      "        [-0.5011, -1.1495,  0.2493,  ..., -0.6730, -1.7478, -0.1120],\n",
      "        [-0.8647, -1.9047,  0.6959,  ..., -0.6853, -0.4685,  0.2469],\n",
      "        [-1.0137, -0.0613,  0.1278,  ..., -0.6153, -1.3629,  0.1716]],\n",
      "       grad_fn=<TransposeBackward0>)), tensor([[-0.5132, -1.8877,  0.7946,  ...,  0.2082, -0.8849,  0.4074],\n",
      "        [-0.5090, -1.9425,  0.2471,  ..., -0.6627, -1.3643,  0.2750],\n",
      "        [-0.5333, -0.4339,  0.0248,  ...,  0.6633, -1.4107,  0.2690],\n",
      "        ...,\n",
      "        [-0.5011, -1.1495,  0.2493,  ..., -0.6730, -1.7478, -0.1120],\n",
      "        [-0.8647, -1.9047,  0.6959,  ..., -0.6853, -0.4685,  0.2469],\n",
      "        [-1.0137, -0.0613,  0.1278,  ..., -0.6153, -1.3629,  0.1716]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[0.7932, 0.2844, 0.1869, 0.7437, 0.9513, 0.1719, 0.9746, 0.8524, 0.9998,\n",
      "         0.7674, 0.5637, 0.5673, 0.7513, 0.7970, 0.4250, 0.1469, 0.8241, 0.7927,\n",
      "         0.4751, 0.6633, 0.3165, 0.1602, 0.5826, 0.7508, 0.4378, 0.4504, 0.9520,\n",
      "         0.8082, 0.1115, 0.3355, 0.7100, 0.6300, 0.7745, 0.8587, 0.9410, 0.3949,\n",
      "         0.3484, 0.2767, 0.9175, 0.8318, 0.1893, 0.6967, 0.5392, 0.3092, 0.1061,\n",
      "         0.9977, 0.5176, 0.2589, 0.7486, 0.9323, 0.9442, 0.8004, 0.5306, 0.8961,\n",
      "         0.2973, 0.9304, 0.7975, 0.7509, 0.5909, 0.1846, 0.3216, 0.2268, 0.5714,\n",
      "         0.3307, 0.6343, 0.4725, 0.8994, 0.8912, 0.3539, 0.5289, 0.5489, 0.7702,\n",
      "         0.9584, 0.1778, 0.3768, 0.5152, 0.9123, 0.8832, 0.2149, 0.5820, 0.9134,\n",
      "         0.7095, 0.5717, 0.8152, 0.3957, 0.1655, 0.9887, 0.8813, 0.3307, 0.9622,\n",
      "         0.5554, 0.3696, 0.7238, 0.6102, 0.5146, 0.5814, 0.9046, 0.7411, 0.8064,\n",
      "         0.2064, 0.2630, 0.2783, 0.9989, 0.1609, 0.4722, 0.2300, 0.8644, 0.1664,\n",
      "         0.8315, 0.6561, 0.9843, 0.2208, 0.9143, 0.3542, 0.2766, 0.6315, 0.2731,\n",
      "         0.9838, 0.3868, 0.1516, 0.3959, 0.6356, 0.7766, 0.4475, 0.3588, 0.6891,\n",
      "         0.9513, 0.7108, 0.8327, 0.3533, 0.7273, 0.4393, 0.2096, 0.7693, 0.2065,\n",
      "         0.5315, 0.3322, 0.3554, 0.3662, 0.6638, 0.7185, 0.7323, 0.7745, 0.3416,\n",
      "         0.5983, 0.7631, 0.1029, 0.8738, 0.7575, 0.7852, 0.7308, 0.5346, 0.5389,\n",
      "         0.9339, 0.3351, 0.1845, 0.8439, 0.4317, 0.8832, 0.2950, 0.4825, 0.1956,\n",
      "         0.3712, 0.5864, 0.4136, 0.3855, 0.1856, 0.6700, 0.6633, 0.5786, 0.1999,\n",
      "         0.8817, 0.2743, 0.2526, 0.8100, 0.7108, 0.4046, 0.9713, 0.5188, 0.2613,\n",
      "         0.7837, 0.2598, 0.4685, 0.8726, 0.3906, 0.7129, 0.7819, 0.5638, 0.2207,\n",
      "         0.1447, 0.8123, 0.7440, 0.8456, 0.4223, 0.6052, 0.5002, 0.6024, 0.9426,\n",
      "         0.2857, 0.3021, 0.4923, 0.6259, 0.7754, 0.7825, 0.4081, 0.3748, 0.6247,\n",
      "         0.5744, 0.8027, 0.8293, 0.2261, 0.1265, 0.7340, 0.7391, 0.5193, 0.4761,\n",
      "         0.7425, 0.2769, 0.9057, 0.1411, 0.4209, 0.7385, 0.2576, 0.2354, 0.8890,\n",
      "         0.1756, 0.3901, 0.4668, 0.1109, 0.4721, 0.8363, 0.9157, 0.8773, 0.3792,\n",
      "         0.3764, 0.1102, 0.7176, 0.5335, 0.1167, 0.9770, 0.2555, 0.1793, 0.5130,\n",
      "         0.7599, 0.9248, 0.1890, 0.5074, 0.9686, 0.8354, 0.7356, 0.6292, 0.6606,\n",
      "         0.8394, 0.4161, 0.6405, 0.8937]])), 'context_position_val': 4, 'dataset': <prior_generation.prior_dataloader.get_dataloader.<locals>.PriorDataset object at 0x7a7735e8fb60>, 'num_workers': 0, 'prefetch_factor': None, 'pin_memory': False, 'pin_memory_device': '', 'timeout': 0, 'worker_init_fn': None, '_DataLoader__multiprocessing_context': None, 'in_order': True, '_dataset_kind': 1, 'batch_size': None, 'drop_last': False, 'sampler': <torch.utils.data.dataloader._InfiniteConstantSampler object at 0x7a7734306d50>, 'batch_sampler': None, 'generator': None, 'collate_fn': <function default_convert at 0x7a77dab9e090>, 'persistent_workers': False, '_DataLoader__initialized': True, '_IterableDataset_len_called': None, '_iterator': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0113, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0321, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0212, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0165, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0178, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0101, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0177, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0173, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0325, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0207, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2000 [00:26<14:52:15, 26.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.019718599319458\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | loss  3.02 |  pos loss   nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 3.02, lr 0.0 \n",
      "-----------------------------------------------------------------------------------------\n",
      "tensor(3.0188, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0162, grad_fn=<MeanBackward0>)\n",
      "tensor(2.9995, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2000 [00:33<18:35:24, 33.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 6. Run Training\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting training on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m losses, positional_losses, val_losses,  model = \u001b[43mtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprior_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Passing the wrapper\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformer_configuration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerators\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_configuration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprior_hyperparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprior_hyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_delimiter_generator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_delimiter_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/thesis/msc-thesis-vasko/src/train.py:110\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(prior_dataloader, criterion, transformer_configuration, generators, training_configuration, prior_hyperparameters, load_path, context_delimiter_generator, device, verbose, save_path, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m losses, positional_losses, val_losses = [], [], []\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     loss, positional_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    111\u001b[39m     losses.append(loss)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/thesis/msc-thesis-vasko/src/train.py:90\u001b[39m, in \u001b[36mtrain.<locals>.train_one_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     88\u001b[39m loss = losses.mean()\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch % aggregate_k_gradients == aggregate_k_gradients - \u001b[32m1\u001b[39m:\n\u001b[32m     92\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/autograd/function.py:300\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBackwardCFunction\u001b[39;00m(_C._FunctionBase, FunctionCtx, _HookMixin):\n\u001b[32m    296\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[33;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m    301\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[33;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m    304\u001b[39m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[32m    305\u001b[39m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 6. Run Training\n",
    "print(f\"Starting training on {device}...\")\n",
    "losses, positional_losses, val_losses,  model = train.train(\n",
    "    prior_dataloader=prior,\n",
    "    criterion=criterion, # Passing the wrapper\n",
    "    transformer_configuration=transformer_configuration,\n",
    "    generators = generators,\n",
    "    training_configuration=training_configuration,\n",
    "    prior_hyperparameters=prior_hyperparameters,\n",
    "    load_path=None,\n",
    "    context_delimiter_generator = context_delimiter_generator,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    save_path=None,\n",
    ")\n",
    "# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.lineplot(x=np.arange(0, len(losses)), y=np.array(losses), label=\"Training\")\n",
    "sns.lineplot(x=np.arange(0, len(losses)), y=np.array(val_losses), label=\"Validation\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_in_dataset = 15\n",
    "train_X, train_Y, y_target = prior.get_datasets_from_prior(9, num_points_in_dataset, 1, **hyperparameters)\n",
    "train_X = train_X.to(device)\n",
    "train_Y = train_Y.to(device)\n",
    "y_target = y_target.to(device)\n",
    "num_training_points = num_points_in_dataset - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75332f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(device)\n",
    "# Set up grid for subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 8)) \n",
    "axes = axes.flatten()\n",
    "\n",
    "for batch_index in range(9):\n",
    "    ax = axes[batch_index] \n",
    "    train_x = train_X[:num_training_points, batch_index, :]\n",
    "    train_y = train_Y[:num_training_points, batch_index]\n",
    "    test_x = train_X[:, batch_index, :]\n",
    "    with torch.no_grad():\n",
    "        logits = model((torch.cat((train_x, test_x)), torch.cat((train_y, torch.zeros(len(test_x), device=device)))), context_pos=num_training_points - 1)\n",
    "\n",
    "        pred_means = model.criterion.mean(logits)\n",
    "        pred_confs = model.criterion.quantile(logits)\n",
    "        pred_means = pred_means[-len(test_x):]\n",
    "        pred_confs = pred_confs[-len(test_x):]\n",
    "        # Plot scatter points for training data\n",
    "        ax.scatter(train_x[..., 0].cpu().numpy(), train_y.cpu().numpy(), label=\"Training Data\")\n",
    "\n",
    "    # Plot model predictions\n",
    "    order_test_x = test_x[:, 0].cpu().argsort()\n",
    "    print(test_x.shape)\n",
    "    print(pred_means.shape)\n",
    "    print(pred_confs.shape)\n",
    "    ax.plot(\n",
    "        test_x[order_test_x, 0].cpu().numpy(),\n",
    "        pred_means[order_test_x].cpu().numpy(),\n",
    "        color='green',\n",
    "        label='pfn'\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        test_x[order_test_x, 0].cpu().numpy(),\n",
    "        pred_confs[order_test_x][:, 0].cpu().numpy(),\n",
    "        pred_confs[order_test_x][:, 1].cpu().numpy(),\n",
    "        alpha=.1,\n",
    "        color='green'\n",
    "    )\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
