{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0ac15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Kaggle. Skipping Git clone and pip install.\n",
      "Added to sys.path: /home/vdakov/Desktop/thesis/msc-thesis-vasko/src\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob - Current Token - Do not delete\n",
    "\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    print(\"Detected Kaggle environment. Starting setup...\")\n",
    "    token = getpass('Paste your GitLab token: ')\n",
    "    username = \"vdakov\" \n",
    "    repo_url = f\"https://{username}:{token}@gitlab.ewi.tudelft.nl/dsait5000/tom-viering/msc-thesis-vasko.git\"\n",
    "    !git clone {repo_url}\n",
    "    if os.path.exists(\"msc-thesis-vasko\"):\n",
    "        os.chdir(\"msc-thesis-vasko\")\n",
    "        \n",
    "    %pip install -r requirements.txt\n",
    "    if os.path.exists(\"notebooks\"):\n",
    "        os.chdir(\"notebooks\")\n",
    "\n",
    "else:\n",
    "    print(\"Not running in Kaggle. Skipping Git clone and pip install.\")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "if os.path.basename(current_dir) == 'notebooks':\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "else:\n",
    "    parent_dir = current_dir\n",
    "\n",
    "src_path = os.path.join(parent_dir, 'src')\n",
    "sys.path.append(src_path)\n",
    "print(f\"Added to sys.path: {src_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502b8f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.encoders as encoders\n",
    "from training_util import get_uniform_single_eval_pos_sampler, get_weighted_single_eval_pos_sampler, get_cosine_schedule_with_warmup\n",
    "import train\n",
    "from criterion.bar_distribution import BarDistribution, get_bucket_limits\n",
    "from models import positional_encodings\n",
    "from prior_generation import gp_prior, gp_lengthscale_prior\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "from samplers.distribution_samplers import DistributionSampler\n",
    "from samplers.distributions import ScaledBernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'definitions': {\n",
    "        'num_features': 1,\n",
    "        'num_outputs': 100,\n",
    "        'sequence_length': 10,\n",
    "        'max_eval_pos': 9\n",
    "    },\n",
    "    'training_configuration': {\n",
    "        'epochs': 500,\n",
    "        'batch_size': 256,\n",
    "        'warmup_epochs': 25,\n",
    "        'steps_per_epoch': 10,\n",
    "        'validation_context_pos': 9,\n",
    "        'sequence_length': 10,\n",
    "        'lr': 0.0001,\n",
    "        'scheduler': 'cosine_scheduler',\n",
    "        'aggregate_k_gradients': 1,\n",
    "        'context_delimiter_sampling': 'uniform',\n",
    "        'context_delimiter_max_eval_pos': 9,\n",
    "        \"num_test_parameters\": 1\n",
    "    },\n",
    "    'transformer_configuration': {\n",
    "        'emsize': 512,\n",
    "        'fuse_x_y': False,\n",
    "        'nlayers': 6,\n",
    "        'num_features': 1,\n",
    "        'nhead': 4,\n",
    "        'nhid': 1024,\n",
    "        'num_outputs': 100,\n",
    "        'dropout': 0.2,\n",
    "        'input_normalization': True,\n",
    "        'encoder_type': 'linear',\n",
    "        'pos_encoder_type': 'none',\n",
    "        'y_encoder_type': 'linear'\n",
    "    },\n",
    "    'prior_configuration': {\n",
    "        'prior_learning': False,\n",
    "        'type': 'gaussian_process_prior',\n",
    "        'hyperparams': {\n",
    "            'kernel': 'rbf',\n",
    "            'length_scale': 0.5,\n",
    "            'output_scale': 1,\n",
    "            'noise_std': 0.001,\n",
    "            'num_features': 1,\n",
    "            'num_outputs': 100\n",
    "        }\n",
    "    },\n",
    "    'criterion_configuration': {\n",
    "        'loss': 'bar_distribution',\n",
    "        'min_y': -5,\n",
    "        'max_y': 5,\n",
    "        'num_buckets': 100\n",
    "    }\n",
    "}\n",
    "\n",
    "import load_config\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# transformer_configuration, training_configuration, criterion, generators, prior, prior_hyperparameters, context_delimiter_generator = load_config.load_config_from_yaml('../src/configs/vanilla_pfn.yaml')\n",
    "transformer_configuration, training_configuration, criterion, generators, prior, prior_hyperparameters, context_delimiter_generator = load_config.parse_config_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16fd6629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': 'rbf',\n",
       " 'length_scale': 0.5,\n",
       " 'output_scale': 1,\n",
       " 'noise_std': 0.001,\n",
       " 'num_features': 1,\n",
       " 'num_outputs': 100}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbb480b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GaussianProcessPriorGenerator.get_batch() got multiple values for argument 'num_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprior\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber_of_datasets\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_points_per_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features_per_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprior_hyperparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/thesis/msc-thesis-vasko/src/prior_generation/prior_generator.py:30\u001b[39m, in \u001b[36mPriorGenerator.visualize_datasets\u001b[39m\u001b[34m(self, number_of_datasets, num_points_per_dataset, num_features_per_dataset, device, **hyperparameter_configuration_kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvisualize_datasets\u001b[39m(\u001b[38;5;28mself\u001b[39m, number_of_datasets, num_points_per_dataset, num_features_per_dataset, device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m, **hyperparameter_configuration_kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     datasets = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_datasets_from_prior\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber_of_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_points_per_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features_per_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhyperparameter_configuration_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     x, y_noisy, y, _ = datasets \n\u001b[32m     32\u001b[39m     x, y_noisy, y,= x.detach().numpy() , y.detach().numpy() , y_noisy.detach().numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/thesis/msc-thesis-vasko/src/prior_generation/prior_generator.py:25\u001b[39m, in \u001b[36mPriorGenerator.get_datasets_from_prior\u001b[39m\u001b[34m(self, number_of_datasets, num_points_per_dataset, num_features_per_dataset, device, **hyperparameter_configuration_kwargs)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_datasets_from_prior\u001b[39m(\u001b[38;5;28mself\u001b[39m, number_of_datasets, num_points_per_dataset, num_features_per_dataset, device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m, **hyperparameter_configuration_kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     x, y_noisy, y, prior_parameters = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber_of_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_points_per_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features_per_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhyperparameter_configuration_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x, y_noisy, y, prior_parameters\n",
      "\u001b[31mTypeError\u001b[39m: GaussianProcessPriorGenerator.get_batch() got multiple values for argument 'num_features'"
     ]
    }
   ],
   "source": [
    "prior.visualize_datasets(number_of_datasets=5, num_points_per_dataset=200, num_features_per_dataset=1, device='cpu', **prior_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d125b87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdakov/.conda/envs/thesis/lib/python3.14/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Total Number of Datasets: 2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/vdakov/Desktop/thesis/msc-thesis-vasko/src/criterion/bar_distribution.py:23: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /pytorch/aten/src/ATen/native/BucketizationUtils.h:32.)\n",
      "  target_sample = torch.searchsorted(self.borders, y) - 1\n",
      "loss  2.32 | pos loss  2.31,  nan,  nan, 2.32, 2.32, 2.32, 2.32,  nan, 2.33,  nan, lr 0.0 val score 2.318786144256592: 100%|██████████| 1/1 [00:26<00:00, 26.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full experiment checkpoint saved to ../results/test-model/checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting training on {device}...\")\n",
    "losses, positional_losses, val_losses,  model = train.train(\n",
    "    prior_dataloader=prior,\n",
    "    criterion=criterion, # Passing the wrapper\n",
    "    transformer_configuration=transformer_configuration,\n",
    "    generators = generators,\n",
    "    training_configuration=training_configuration,\n",
    "    prior_hyperparameters=prior_hyperparameters,\n",
    "    load_path=None,\n",
    "    context_delimiter_generator = context_delimiter_generator,\n",
    "    save_folder=\"../results\",\n",
    "    experiment_name=\"test-model\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.lineplot(x=np.arange(0, len(losses)), y=np.array(losses), label=\"Training\")\n",
    "sns.lineplot(x=np.arange(0, len(losses)), y=np.array(val_losses), label=\"Validation\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_in_dataset = 15\n",
    "train_X, train_Y, y_target = prior.get_datasets_from_prior(9, num_points_in_dataset, 1, **hyperparameters)\n",
    "train_X = train_X.to(device)\n",
    "train_Y = train_Y.to(device)\n",
    "y_target = y_target.to(device)\n",
    "num_training_points = num_points_in_dataset - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75332f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "# Set up grid for subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 8)) \n",
    "axes = axes.flatten()\n",
    "\n",
    "for batch_index in range(9):\n",
    "    ax = axes[batch_index] \n",
    "    train_x = train_X[:num_training_points, batch_index, :]\n",
    "    train_y = train_Y[:num_training_points, batch_index]\n",
    "    test_x = train_X[:, batch_index, :]\n",
    "    with torch.no_grad():\n",
    "        logits = model((torch.cat((train_x, test_x)), torch.cat((train_y, torch.zeros(len(test_x), device=device)))), context_pos=num_training_points - 1)\n",
    "\n",
    "        pred_means = model.criterion.mean(logits)\n",
    "        pred_confs = model.criterion.quantile(logits)\n",
    "        pred_means = pred_means[-len(test_x):]\n",
    "        pred_confs = pred_confs[-len(test_x):]\n",
    "        # Plot scatter points for training data\n",
    "        ax.scatter(train_x[..., 0].cpu().numpy(), train_y.cpu().numpy(), label=\"Training Data\")\n",
    "\n",
    "    # Plot model predictions\n",
    "    order_test_x = test_x[:, 0].cpu().argsort()\n",
    "    ax.plot(\n",
    "        test_x[order_test_x, 0].cpu().numpy(),\n",
    "        pred_means[order_test_x].cpu().numpy(),\n",
    "        color='green',\n",
    "        label='pfn'\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        test_x[order_test_x, 0].cpu().numpy(),\n",
    "        pred_confs[order_test_x][:, 0].cpu().numpy(),\n",
    "        pred_confs[order_test_x][:, 1].cpu().numpy(),\n",
    "        alpha=.1,\n",
    "        color='green'\n",
    "    )\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6073d2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdakov/.conda/envs/thesis/lib/python3.14/site-packages/torch/distributions/distribution.py:62: UserWarning: <class 'samplers.distributions.ScaledBernoulli'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import models.encoders as encoders\n",
    "from training_util import get_uniform_single_eval_pos_sampler, get_weighted_single_eval_pos_sampler, get_cosine_schedule_with_warmup\n",
    "import train\n",
    "from criterion.bar_distribution import BarDistribution, get_bucket_limits\n",
    "from models import positional_encodings\n",
    "from prior_generation import gp_prior, gp_lengthscale_prior\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "epochs = 500\n",
    "batch_size =  256\n",
    "warmup_epochs = 25\n",
    "steps_per_epoch = 10\n",
    "lr = 0.0001\n",
    "sequence_length = 25\n",
    "emsize = 512\n",
    "fuse_x_y = False\n",
    "nlayers = 6\n",
    "nhead = 4\n",
    "nhid = 1024\n",
    "dropout = 0.2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_buckets = 5\n",
    "min_y = 0\n",
    "max_y = 1\n",
    "num_features = 1\n",
    "num_outputs = 5\n",
    "my_prior_dist = ScaledBernoulli(0.4, 0.6)\n",
    "sampler = DistributionSampler(my_prior_dist)\n",
    "prior_hyperparameters = {'num_features': num_features, 'num_outputs': num_outputs, 'device': device, 'kernel': \"rbf\", \"length_scale_sampling\": sampler}\n",
    "input_normalization = True\n",
    "aggregate_k_gradients=1\n",
    "encoder_type = 'linear'  # 'linear' or 'mlp'\n",
    "y_encoder_type = 'linear'\n",
    "pos_encoder_type = 'none'  # 'sinus', 'learned', 'none'\n",
    "scheduler = get_cosine_schedule_with_warmup\n",
    "prior_prediction = True\n",
    "num_test_parameters = 1\n",
    "validation_context_pos = sequence_length\n",
    "\n",
    "def get_encoder_generator(encoder):\n",
    "        if encoder == 'linear':\n",
    "            encoder_generator = encoders.LinearEncoder\n",
    "        elif encoder == 'mlp':\n",
    "            encoder_generator = encoders.MLPEncoder\n",
    "        else:\n",
    "            raise NotImplementedError(f'A {encoder} encoder is not valid.')\n",
    "        return encoder_generator\n",
    "\n",
    "encoder_generator = get_encoder_generator(encoder_type)\n",
    "y_encoder_generator = get_encoder_generator(y_encoder_type)\n",
    "\n",
    "if pos_encoder_type== 'sinus':\n",
    "    pos_encoder_generator = positional_encodings.PositionalEncoding\n",
    "elif pos_encoder_type == 'learned':\n",
    "    pos_encoder_generator = positional_encodings.LearnedPositionalEncoding\n",
    "else:\n",
    "    pos_encoder_generator = positional_encodings.NoPositionalEncoding\n",
    "    \n",
    "permutation_invariant_max_eval_pos = sequence_length - 1\n",
    "permutation_invariant_sampling = 'uniform'\n",
    "\n",
    "if permutation_invariant_max_eval_pos is not None:\n",
    "    if permutation_invariant_sampling == 'weighted':\n",
    "        get_sampler = get_weighted_single_eval_pos_sampler\n",
    "    elif permutation_invariant_sampling == 'uniform':\n",
    "        get_sampler = get_uniform_single_eval_pos_sampler\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    \n",
    "context_delimiter_generator = get_sampler(permutation_invariant_max_eval_pos)\n",
    "context_delimiter_generator = lambda : sequence_length\n",
    "\n",
    "transformer_configuration = (emsize, nhead, nhid, nlayers, dropout, num_features, num_outputs, input_normalization, y_encoder_generator, sequence_length, fuse_x_y, prior_prediction, num_test_parameters) \n",
    "training_configuration = (epochs, steps_per_epoch, batch_size, sequence_length, lr, warmup_epochs, aggregate_k_gradients, scheduler, prior_prediction, validation_context_pos)\n",
    "generators = (encoder_generator, y_encoder_generator, pos_encoder_generator)\n",
    "# prior = gp_prior.GaussianProcessPriorGenerator()\n",
    "prior = gp_lengthscale_prior.GaussianProcessHyperPriorGenerator()\n",
    "criterion = BarDistribution(borders=get_bucket_limits(num_buckets, full_range=(min_y, max_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52042e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu...\n",
      "Using cpu:0 device\n",
      "1\n",
      "1\n",
      "Total Number of Datasets: 1280000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss -0.00 | pos loss   nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,-0.00, lr 0.0 val score nan:   0%|          | 1/500 [01:15<10:28:03, 75.52s/it]"
     ]
    }
   ],
   "source": [
    "# 6. Run Training\n",
    "print(f\"Starting training on {device}...\")\n",
    "losses, positional_losses, val_losses,  model = train.train(\n",
    "    prior_dataloader=prior,\n",
    "    criterion=criterion, # Passing the wrapper\n",
    "    transformer_configuration=transformer_configuration,\n",
    "    generators = generators,\n",
    "    training_configuration=training_configuration,\n",
    "    prior_hyperparameters=prior_hyperparameters,\n",
    "    load_path=None,\n",
    "    context_delimiter_generator = context_delimiter_generator,\n",
    "    device=device,\n",
    "    save_path=None,\n",
    ")\n",
    "# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0424c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_in_dataset = 100\n",
    "hyperparameters = { 'kernel': \"rbf\", 'length_scale': 0.4, \"length_scale_sampling\": DistributionSampler(dist.Uniform(low=0.4, high=0.401))}\n",
    "prior.visualize_datasets(number_of_datasets=5, num_points_per_dataset=num_points_in_dataset, num_features_per_dataset=1, device='cpu', **hyperparameters)\n",
    "train_X, train_Y, y_target, lengthscale = prior.get_datasets_from_prior(1, num_points_in_dataset, 1, **hyperparameters)\n",
    "print(lengthscale)\n",
    "train_X = train_X.to(device)\n",
    "train_Y = train_Y.to(device)\n",
    "y_target = y_target.to(device)\n",
    "num_training_points = num_points_in_dataset\n",
    "\n",
    "import seaborn as sns \n",
    "model = model.to(device)\n",
    "train_x = train_X\n",
    "train_y = train_Y\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model((train_x, train_y), context_pos=num_training_points)\n",
    "    outputs = torch.exp(torch.log_softmax(logits, -1))\n",
    "outputs = torch.squeeze(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Get all borders first (don't slice yet)\n",
    "borders_all = model.criterion.borders.detach().cpu().numpy()\n",
    "\n",
    "# 2. Calculate the width of each bin\n",
    "widths = np.diff(borders_all)\n",
    "\n",
    "# 3. Define X coordinates (using the left edge of each bin is safer for alignment)\n",
    "left_edges = borders_all[:-1]\n",
    "\n",
    "# 4. Get Values and Mask\n",
    "values = torch.squeeze(outputs).detach().cpu().numpy()\n",
    "threshold = 0.001\n",
    "mask = values > threshold\n",
    "\n",
    "# 5. Plot with specific widths and 'edge' alignment\n",
    "# align='edge' means the bar starts at x and extends to the right by 'width'\n",
    "plt.bar(left_edges[mask], values[mask], width=widths[mask], align='edge', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel(\"Lengthscale\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(f\"Focus on Significant Factors (> {threshold})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fdbdc325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Lengthscale: 0.4\n",
      "Recovered Lengthscale: 0.4060\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rbf_kernel(X1, X2, lengthscale, variance=1.0):\n",
    "    dists = cdist(X1, X2, metric='sqeuclidean')\n",
    "    return variance * np.exp(-0.5 * dists / (lengthscale ** 2))\n",
    "\n",
    "def nll_fn(theta, X, y, noise_variance=1e-4):\n",
    "    lengthscale = np.exp(theta[0]) \n",
    "    \n",
    "    n = len(y)\n",
    "    K = rbf_kernel(X, X, lengthscale) + noise_variance * np.eye(n)\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(K)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.inf\n",
    "        \n",
    "    # Solve for alpha = K^-1 y = L^TL y\n",
    "    alpha = np.linalg.solve(L.T, np.linalg.solve(L, y))\n",
    "    \n",
    "    \n",
    "    data_fit = 0.5 * y.T @ alpha\n",
    "    \n",
    "    # 2. Complexity Term (log determinant)\n",
    "    complexity = np.sum(np.log(np.diag(L)))\n",
    "    \n",
    "    # 3. Constant term (optional for optimization)\n",
    "    constant = 0.5 * n * np.log(2 * np.pi)\n",
    "    \n",
    "    return (data_fit + complexity + constant).item()\n",
    "\n",
    "# --- Simulation ---\n",
    "\n",
    "# Generate dummy data with a TRUE lengthscale of 1.5\n",
    "true_lengthscale = 0.4\n",
    "X = np.random.uniform(0, 1, size=(10, 1))\n",
    "K_true = rbf_kernel(X, X, lengthscale=true_lengthscale)\n",
    "y = np.random.multivariate_normal(mean=np.zeros(len(X)), cov=K_true).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Initial guess (heuristic: random or std dev of X)\n",
    "initial_log_ls = np.log(1.0) \n",
    "\n",
    "# Optimization\n",
    "res = minimize(nll_fn, x0=[initial_log_ls], args=(X, y), method='L-BFGS-B')\n",
    "\n",
    "recovered_ls = np.exp(res.x[0])\n",
    "\n",
    "print(f\"True Lengthscale: {true_lengthscale}\")\n",
    "print(f\"Recovered Lengthscale: {recovered_ls:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
