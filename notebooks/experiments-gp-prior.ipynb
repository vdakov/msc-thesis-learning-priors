{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ac15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Securely input your token\n",
    "token = getpass('Paste your GitLab token: ')\n",
    "\n",
    "# Set the repository URL\n",
    "# repo_url = \"https://oauth2:\" + token + \"@gitlab.ewi.tudelft.nl/dsait5000/tom-viering/msc-thesis-vasko.git\"\n",
    "username = \"vdakov\" \n",
    "\n",
    "# Syntax: https://<username>:<token>@<domain>/...\n",
    "repo_url = f\"https://{username}:{token}@gitlab.ewi.tudelft.nl/dsait5000/tom-viering/msc-thesis-vasko.git\"\n",
    "\n",
    "!git clone {repo_url}\n",
    "\n",
    "# Clone the repository\n",
    "# !git clone {repo_url}\n",
    "# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob\n",
    "# Verify clone\n",
    "os.chdir(\"msc-thesis-vasko\")\n",
    "%pip install -r requirements.txt\n",
    "os.chdir(\"notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c539e1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: /home/vdakov/Desktop/thesis/msc-thesis-vasko\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(os.path.join(parent_dir, 'src'))\n",
    "print(f\"Added to sys.path: {parent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f689e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Distribution\n",
    "\n",
    "class DistributionSampler:\n",
    "    def __init__(self, distribution: Distribution):\n",
    "        \"\"\"\n",
    "        :param distribution: An instantiated torch.distributions object \n",
    "                             (e.g., Uniform(0.1, 3.0), Gamma(2.0, 2.0))\n",
    "        \"\"\"\n",
    "        self.distribution = distribution\n",
    "\n",
    "    def sample(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the distribution with the given batch size.\n",
    "        Returns a tensor of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # PyTorch distributions expect a tuple/torch.Size for sampling\n",
    "        return self.distribution.sample(torch.Size([batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Process Prior\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import models.encoders as encoders\n",
    "from training_util import get_uniform_single_eval_pos_sampler, get_weighted_single_eval_pos_sampler, get_cosine_schedule_with_warmup\n",
    "import train\n",
    "from criterion.bar_distribution import BarDistribution, get_bucket_limits\n",
    "from models import positional_encodings\n",
    "from prior_generation import gp_prior, gp_lengthscale_prior\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "epochs = 3\n",
    "batch_size =  256\n",
    "warmup_epochs = 25\n",
    "steps_per_epoch = 10\n",
    "lr = 0.0001\n",
    "sequence_length = 10\n",
    "emsize = 512\n",
    "fuse_x_y = False\n",
    "nlayers = 6\n",
    "nhead = 4\n",
    "nhid = 1024\n",
    "dropout = 0.2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_buckets = 100\n",
    "min_y = -10\n",
    "max_y = 10\n",
    "num_features = 1\n",
    "num_outputs = 100\n",
    "my_prior_dist = dist.Uniform(low=0.1, high=1)\n",
    "\n",
    "sampler = DistributionSampler(my_prior_dist)\n",
    "# prior_hyperparameters = {'num_features': num_features, 'num_outputs': num_outputs, 'device': device, 'kernel': \"rbf\", 'length_scale': 0.5}\n",
    "prior_hyperparameters = {'num_features': num_features, 'num_outputs': num_outputs, 'device': device, 'kernel': \"rbf\", 'length_scale': 0.5, \"length_scale_sampling\": sampler}\n",
    "input_normalization = True\n",
    "aggregate_k_gradients=1\n",
    "encoder_type = 'linear'  # 'linear' or 'mlp'\n",
    "y_encoder_type = 'linear'\n",
    "pos_encoder_type = 'none'  # 'sinus', 'learned', 'none'\n",
    "scheduler = get_cosine_schedule_with_warmup\n",
    "prior_prediction = False\n",
    "num_test_parameters = 1\n",
    "\n",
    "def get_encoder_generator(encoder):\n",
    "        if encoder == 'linear':\n",
    "            encoder_generator = encoders.LinearEncoder\n",
    "        elif encoder == 'mlp':\n",
    "            encoder_generator = encoders.MLPEncoder\n",
    "        else:\n",
    "            raise NotImplementedError(f'A {encoder} encoder is not valid.')\n",
    "        return encoder_generator\n",
    "\n",
    "encoder_generator = get_encoder_generator(encoder_type)\n",
    "y_encoder_generator = get_encoder_generator(y_encoder_type)\n",
    "\n",
    "if pos_encoder_type== 'sinus':\n",
    "    pos_encoder_generator = positional_encodings.PositionalEncoding\n",
    "elif pos_encoder_type == 'learned':\n",
    "    pos_encoder_generator = positional_encodings.LearnedPositionalEncoding\n",
    "else:\n",
    "    pos_encoder_generator = positional_encodings.NoPositionalEncoding\n",
    "    \n",
    "permutation_invariant_max_eval_pos = sequence_length - 1\n",
    "permutation_invariant_sampling = 'uniform'\n",
    "\n",
    "if permutation_invariant_max_eval_pos is not None:\n",
    "    if permutation_invariant_sampling == 'weighted':\n",
    "        get_sampler = get_weighted_single_eval_pos_sampler\n",
    "    elif permutation_invariant_sampling == 'uniform':\n",
    "        get_sampler = get_uniform_single_eval_pos_sampler\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    \n",
    "context_delimiter_generator = get_sampler(permutation_invariant_max_eval_pos)\n",
    "\n",
    "transformer_configuration = (emsize, nhead, nhid, nlayers, dropout, num_features, num_outputs, input_normalization, y_encoder_generator, sequence_length, fuse_x_y, prior_prediction, num_test_parameters = 1) \n",
    "training_configuration = (epochs, steps_per_epoch, batch_size, sequence_length, lr, warmup_epochs, aggregate_k_gradients, scheduler, prior_prediction)\n",
    "generators = (encoder_generator, y_encoder_generator, pos_encoder_generator)\n",
    "prior = gp_prior.GaussianProcessPriorGenerator()\n",
    "# prior = gp_lengthscale_prior.GaussianProcessHyperPriorGenerator()\n",
    "print(prior.name)\n",
    "criterion = BarDistribution(borders=get_bucket_limits(num_buckets, full_range=(min_y, max_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb480b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = { 'kernel': \"rbf\", 'length_scale': 0.5}\n",
    "prior.visualize_datasets(number_of_datasets=5, num_points_per_dataset=200, num_features_per_dataset=1, device='cpu', **hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d125b87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu...\n",
      "Using cpu:0 device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdakov/.conda/envs/thesis/lib/python3.14/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/vdakov/.conda/envs/thesis/lib/python3.14/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 6. Run Training\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting training on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m losses, positional_losses, val_losses,  model = \u001b[43mtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprior_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Passing the wrapper\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformer_configuration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerators\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_configuration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprior_hyperparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprior_hyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_delimiter_generator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_delimiter_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/thesis/msc-thesis-vasko/src/train.py:57\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(prior_dataloader, criterion, transformer_configuration, generators, training_configuration, prior_hyperparameters, load_path, context_delimiter_generator, device, verbose, save_path, **kwargs)\u001b[39m\n\u001b[32m     53\u001b[39m     model.load_state_dict(load_path)\n\u001b[32m     54\u001b[39m model.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m optimizer = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m scheduler = scheduler(optimizer, warmup_epochs, epochs)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_one_epoch\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/optim/adamw.py:37\u001b[39m, in \u001b[36mAdamW.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     24\u001b[39m     params: ParamsT,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     fused: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     36\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbetas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/optim/adam.py:101\u001b[39m, in \u001b[36mAdam.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor betas[1] must be 1-element\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m defaults = {\n\u001b[32m     89\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: lr,\n\u001b[32m     90\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m: betas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdecoupled_weight_decay\u001b[39m\u001b[33m\"\u001b[39m: decoupled_weight_decay,\n\u001b[32m    100\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/optim/optimizer.py:401\u001b[39m, in \u001b[36mOptimizer.__init__\u001b[39m\u001b[34m(self, params, defaults)\u001b[39m\n\u001b[32m    398\u001b[39m     param_groups = [{\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: param_groups}]\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._warned_capturable_if_run_uncaptured = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/_compile.py:46\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m disable_fn = \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[33m\"\u001b[39m\u001b[33m__dynamo_disable\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# We can safely turn off functools.wraps here because the inner\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# already wraps fn in the outer scope.\u001b[39;00m\n\u001b[32m     50\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/_dynamo/__init__.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     aot_compile,\n\u001b[32m     15\u001b[39m     config,\n\u001b[32m     16\u001b[39m     convert_frame,\n\u001b[32m     17\u001b[39m     eval_frame,\n\u001b[32m     18\u001b[39m     functional_export,\n\u001b[32m     19\u001b[39m     resume_execution,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/_dynamo/aot_compile.py:15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprecompile_context\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrecompileContext\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_frame\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Hooks\n\u001b[32m     19\u001b[39m log = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/_dynamo/convert_frame.py:57\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CallbackTrigger\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/_dynamo/symbolic_convert.py:103\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfuncname_cache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_funcname\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GuardBuilder, install_guard\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput_graph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphCompileReason, OutputGraph\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolyfills\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m impl_CONTAINS_OP_fallback\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreplay_record\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DummyModule, ExecutionRecorder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/_dynamo/output_graph.py:101\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdevice_interface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_interface_for_device\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     95\u001b[39m     BackendCompilerFailed,\n\u001b[32m     96\u001b[39m     exceptions_allowed_to_be_fallback,\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m     unimplemented_v2_with_warning,\n\u001b[32m    100\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_deduplication\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_graph_deduplication\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_region_tracker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphRegionTracker\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GuardBuilder, install_guard\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/_dynamo/graph_deduplication.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreductions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StorageWeakRef\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ordered_set\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedSet\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_region_tracker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Node, Region\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _detect_cycles, _get_flat_args, _get_flat_args_unique\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Represents an index into the region\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# to select a node and then\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# an index into that node's\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# flattened arguments\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1371\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n\u001b[32m   1369\u001b[39m     module = sys.modules.get(name, _NEEDS_LOADING)\n\u001b[32m   1370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m _NEEDS_LOADING:\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_find_and_load_unlocked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimport_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;66;03m# Optimization: only call _bootstrap._lock_unlock_module() if\u001b[39;00m\n\u001b[32m   1374\u001b[39m \u001b[38;5;66;03m# module.__spec__._initializing is True.\u001b[39;00m\n\u001b[32m   1375\u001b[39m \u001b[38;5;66;03m# NOTE: because of this, initializing must be set *before*\u001b[39;00m\n\u001b[32m   1376\u001b[39m \u001b[38;5;66;03m# putting the new module in sys.modules.\u001b[39;00m\n\u001b[32m   1377\u001b[39m _lock_unlock_module(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1342\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n\u001b[32m   1340\u001b[39m     parent_spec._uninitialized_submodules.append(child)\n\u001b[32m   1341\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1342\u001b[39m     module = \u001b[43m_load_unlocked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1344\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parent_spec:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:938\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n\u001b[32m    935\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mmissing loader\u001b[39m\u001b[33m'\u001b[39m, name=spec.name)\n\u001b[32m    936\u001b[39m         \u001b[38;5;66;03m# A namespace package so do nothing.\u001b[39;00m\n\u001b[32m    937\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m938\u001b[39m         \u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    940\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:755\u001b[39m, in \u001b[36m_LoaderBasics.exec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexec_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module):\n\u001b[32m    754\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute the module.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m755\u001b[39m     code = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    756\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    757\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcannot load module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m when \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    758\u001b[39m                           \u001b[33m'\u001b[39m\u001b[33mget_code() returns None\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:888\u001b[39m, in \u001b[36mSourceLoader.get_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n\u001b[32m    885\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    886\u001b[39m                 _bootstrap._verbose_message(\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m matches \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m, bytecode_path,\n\u001b[32m    887\u001b[39m                                             source_path)\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_bytecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mbytecode_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbytecode_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_bytes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    892\u001b[39m     source_bytes = \u001b[38;5;28mself\u001b[39m.get_data(source_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:511\u001b[39m, in \u001b[36m_compile_bytecode\u001b[39m\u001b[34m(data, name, bytecode_path, source_path)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_bytecode\u001b[39m(data, name=\u001b[38;5;28;01mNone\u001b[39;00m, bytecode_path=\u001b[38;5;28;01mNone\u001b[39;00m, source_path=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    510\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compile bytecode as found in a pyc.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     code = \u001b[43mmarshal\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(code, _code_type):\n\u001b[32m    513\u001b[39m         _bootstrap._verbose_message(\u001b[33m'\u001b[39m\u001b[33mcode object from \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m'\u001b[39m, bytecode_path)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 6. Run Training\n",
    "print(f\"Starting training on {device}...\")\n",
    "losses, positional_losses, val_losses,  model = train.train(\n",
    "    prior_dataloader=prior,\n",
    "    criterion=criterion, # Passing the wrapper\n",
    "    transformer_configuration=transformer_configuration,\n",
    "    generators = generators,\n",
    "    training_configuration=training_configuration,\n",
    "    prior_hyperparameters=prior_hyperparameters,\n",
    "    load_path=None,\n",
    "    context_delimiter_generator = context_delimiter_generator,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    save_path=None,\n",
    ")\n",
    "# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.lineplot(x=np.arange(0, len(losses)), y=np.array(losses), label=\"Training\")\n",
    "sns.lineplot(x=np.arange(0, len(losses)), y=np.array(val_losses), label=\"Validation\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_in_dataset = 15\n",
    "train_X, train_Y, y_target = prior.get_datasets_from_prior(9, num_points_in_dataset, 1, **hyperparameters)\n",
    "train_X = train_X.to(device)\n",
    "train_Y = train_Y.to(device)\n",
    "y_target = y_target.to(device)\n",
    "num_training_points = num_points_in_dataset - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75332f29",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mmodel\u001b[49m.to(device)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Set up grid for subplots\u001b[39;00m\n\u001b[32m      3\u001b[39m fig, axes = plt.subplots(\u001b[32m3\u001b[39m, \u001b[32m3\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m8\u001b[39m)) \n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model = model.to(device)\n",
    "# Set up grid for subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 8)) \n",
    "axes = axes.flatten()\n",
    "\n",
    "for batch_index in range(9):\n",
    "    ax = axes[batch_index] \n",
    "    train_x = train_X[:num_training_points, batch_index, :]\n",
    "    train_y = train_Y[:num_training_points, batch_index]\n",
    "    test_x = train_X[:, batch_index, :]\n",
    "    with torch.no_grad():\n",
    "        logits = model((torch.cat((train_x, test_x)), torch.cat((train_y, torch.zeros(len(test_x), device=device)))), context_pos=num_training_points - 1)\n",
    "\n",
    "        pred_means = model.criterion.mean(logits)\n",
    "        pred_confs = model.criterion.quantile(logits)\n",
    "        pred_means = pred_means[-len(test_x):]\n",
    "        pred_confs = pred_confs[-len(test_x):]\n",
    "        # Plot scatter points for training data\n",
    "        ax.scatter(train_x[..., 0].cpu().numpy(), train_y.cpu().numpy(), label=\"Training Data\")\n",
    "\n",
    "    # Plot model predictions\n",
    "    order_test_x = test_x[:, 0].cpu().argsort()\n",
    "    ax.plot(\n",
    "        test_x[order_test_x, 0].cpu().numpy(),\n",
    "        pred_means[order_test_x].cpu().numpy(),\n",
    "        color='green',\n",
    "        label='pfn'\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        test_x[order_test_x, 0].cpu().numpy(),\n",
    "        pred_confs[order_test_x][:, 0].cpu().numpy(),\n",
    "        pred_confs[order_test_x][:, 1].cpu().numpy(),\n",
    "        alpha=.1,\n",
    "        color='green'\n",
    "    )\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6073d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import models.encoders as encoders\n",
    "from training_util import get_uniform_single_eval_pos_sampler, get_weighted_single_eval_pos_sampler, get_cosine_schedule_with_warmup\n",
    "import train\n",
    "from criterion.bar_distribution import BarDistribution, get_bucket_limits\n",
    "from models import positional_encodings\n",
    "from prior_generation import gp_prior, gp_lengthscale_prior\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "epochs = 10\n",
    "batch_size =  256\n",
    "warmup_epochs = 25\n",
    "steps_per_epoch = 10\n",
    "lr = 0.0001\n",
    "sequence_length = 10\n",
    "emsize = 512\n",
    "fuse_x_y = False\n",
    "nlayers = 6\n",
    "nhead = 4\n",
    "nhid = 1024\n",
    "dropout = 0.2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_buckets = 100\n",
    "min_y = -10\n",
    "max_y = 10\n",
    "num_features = 1\n",
    "num_outputs = 100\n",
    "my_prior_dist = dist.Uniform(low=0.4, high=0.6)\n",
    "sampler = DistributionSampler(my_prior_dist)\n",
    "prior_hyperparameters = {'num_features': num_features, 'num_outputs': num_outputs, 'device': device, 'kernel': \"rbf\", \"length_scale_sampling\": sampler}\n",
    "input_normalization = True\n",
    "aggregate_k_gradients=1\n",
    "encoder_type = 'linear'  # 'linear' or 'mlp'\n",
    "y_encoder_type = 'linear'\n",
    "pos_encoder_type = 'none'  # 'sinus', 'learned', 'none'\n",
    "scheduler = get_cosine_schedule_with_warmup\n",
    "prior_prediction = True\n",
    "num_test_parameters = 1\n",
    "\n",
    "def get_encoder_generator(encoder):\n",
    "        if encoder == 'linear':\n",
    "            encoder_generator = encoders.LinearEncoder\n",
    "        elif encoder == 'mlp':\n",
    "            encoder_generator = encoders.MLPEncoder\n",
    "        else:\n",
    "            raise NotImplementedError(f'A {encoder} encoder is not valid.')\n",
    "        return encoder_generator\n",
    "\n",
    "encoder_generator = get_encoder_generator(encoder_type)\n",
    "y_encoder_generator = get_encoder_generator(y_encoder_type)\n",
    "\n",
    "if pos_encoder_type== 'sinus':\n",
    "    pos_encoder_generator = positional_encodings.PositionalEncoding\n",
    "elif pos_encoder_type == 'learned':\n",
    "    pos_encoder_generator = positional_encodings.LearnedPositionalEncoding\n",
    "else:\n",
    "    pos_encoder_generator = positional_encodings.NoPositionalEncoding\n",
    "    \n",
    "permutation_invariant_max_eval_pos = sequence_length - 1\n",
    "permutation_invariant_sampling = 'uniform'\n",
    "\n",
    "if permutation_invariant_max_eval_pos is not None:\n",
    "    if permutation_invariant_sampling == 'weighted':\n",
    "        get_sampler = get_weighted_single_eval_pos_sampler\n",
    "    elif permutation_invariant_sampling == 'uniform':\n",
    "        get_sampler = get_uniform_single_eval_pos_sampler\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    \n",
    "context_delimiter_generator = get_sampler(permutation_invariant_max_eval_pos)\n",
    "\n",
    "transformer_configuration = (emsize, nhead, nhid, nlayers, dropout, num_features, num_outputs, input_normalization, y_encoder_generator, sequence_length, fuse_x_y, prior_prediction, num_test_parameters) \n",
    "training_configuration = (epochs, steps_per_epoch, batch_size, sequence_length, lr, warmup_epochs, aggregate_k_gradients, scheduler, prior_prediction)\n",
    "generators = (encoder_generator, y_encoder_generator, pos_encoder_generator)\n",
    "# prior = gp_prior.GaussianProcessPriorGenerator()\n",
    "prior = gp_lengthscale_prior.GaussianProcessHyperPriorGenerator()\n",
    "criterion = BarDistribution(borders=get_bucket_limits(num_buckets, full_range=(min_y, max_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52042e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdakov/.conda/envs/thesis/lib/python3.14/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-03 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/vdakov/.conda/envs/thesis/lib/python3.14/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu...\n",
      "Using cpu:0 device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss -1.54 | pos loss   nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,-1.54, lr 3.6e-05val score 3.0787858963012695: 100%|██████████| 10/10 [03:46<00:00, 22.64s/it]              \n"
     ]
    }
   ],
   "source": [
    "# 6. Run Training\n",
    "print(f\"Starting training on {device}...\")\n",
    "losses, positional_losses, val_losses,  model = train.train(\n",
    "    prior_dataloader=prior,\n",
    "    criterion=criterion, # Passing the wrapper\n",
    "    transformer_configuration=transformer_configuration,\n",
    "    generators = generators,\n",
    "    training_configuration=training_configuration,\n",
    "    prior_hyperparameters=prior_hyperparameters,\n",
    "    load_path=None,\n",
    "    context_delimiter_generator = context_delimiter_generator,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    save_path=None,\n",
    ")\n",
    "# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0424c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_in_dataset = 15\n",
    "hyperparameters = { 'kernel': \"rbf\", 'length_scale': 0.5, \"length_scale_sampling\": DistributionSampler(dist.Uniform(low=0.5, high=0.51))}\n",
    "train_X, train_Y, y_target, lengthscale = prior.get_datasets_from_prior(1, num_points_in_dataset, 1, **hyperparameters)\n",
    "train_X = train_X.to(device)\n",
    "train_Y = train_Y.to(device)\n",
    "y_target = y_target.to(device)\n",
    "num_training_points = num_points_in_dataset - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46da943d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 1]) torch.Size([10, 1])\n",
      "torch.Size([2, 1, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.2310e-04, 1.3353e-04, 2.1978e-04, 1.1379e-04, 2.8868e-04, 1.4440e-04,\n",
       "         2.2002e-04, 1.7540e-04, 2.9311e-04, 1.7481e-04, 2.1917e-04, 2.8738e-04,\n",
       "         1.3248e-04, 9.1900e-05, 2.0902e-04, 2.2789e-04, 1.5159e-04, 1.8090e-04,\n",
       "         1.3317e-04, 1.9534e-04, 2.1000e-04, 1.7705e-04, 3.4093e-04, 3.2947e-04,\n",
       "         1.2682e-04, 1.4580e-04, 1.8056e-04, 1.7978e-04, 1.7112e-04, 2.2638e-04,\n",
       "         1.2841e-04, 1.5431e-04, 1.8155e-04, 1.1404e-04, 1.7390e-04, 1.9910e-04,\n",
       "         3.0859e-04, 2.3569e-04, 1.0981e-04, 2.1062e-04, 2.1902e-04, 2.3397e-04,\n",
       "         2.1157e-04, 1.9905e-04, 1.5288e-04, 2.9350e-04, 2.0228e-04, 2.5069e-04,\n",
       "         2.3025e-04, 2.3775e-04, 1.3805e-04, 1.8329e-04, 9.8002e-01, 1.3857e-04,\n",
       "         1.1154e-04, 2.0256e-04, 1.9245e-04, 4.2368e-04, 2.9499e-04, 2.5232e-04,\n",
       "         2.3234e-04, 1.8521e-04, 2.6889e-04, 1.3686e-04, 1.8247e-04, 2.2473e-04,\n",
       "         2.9452e-04, 1.8805e-04, 1.3200e-04, 1.9270e-04, 1.9809e-04, 1.2168e-04,\n",
       "         1.3833e-04, 1.4871e-04, 2.2260e-04, 1.2035e-04, 1.6041e-04, 1.7111e-04,\n",
       "         3.7351e-04, 2.4265e-04, 2.1514e-04, 2.2520e-04, 1.7151e-04, 1.2036e-04,\n",
       "         2.1675e-04, 1.8075e-04, 2.4795e-04, 2.1960e-04, 2.8963e-04, 1.6958e-04,\n",
       "         2.1374e-04, 1.4491e-04, 1.5659e-04, 1.8046e-04, 1.1540e-04, 2.0308e-04,\n",
       "         3.3221e-04, 2.0932e-04, 3.6815e-04, 1.7243e-04]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "model = model.to(device)\n",
    "train_x = train_X[:num_training_points]\n",
    "train_y = train_Y[:num_training_points]\n",
    "\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "with torch.no_grad():\n",
    "    logits = model((train_x, train_y), context_pos=num_training_points - 1)\n",
    "    print(logits.shape)\n",
    "    outputs = torch.exp(torch.log_softmax(logits, -1))\n",
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859fecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.          -9.8         -9.6         -9.4         -9.2\n",
      "  -9.          -8.8         -8.6         -8.4         -8.2\n",
      "  -8.          -7.8         -7.6         -7.3999996   -7.2\n",
      "  -7.          -6.8         -6.6         -6.3999996   -6.2\n",
      "  -6.          -5.7999997   -5.6         -5.4         -5.2\n",
      "  -5.          -4.7999997   -4.6         -4.4         -4.2\n",
      "  -4.          -3.7999997   -3.6         -3.4         -3.1999998\n",
      "  -3.          -2.7999997   -2.6         -2.4         -2.1999998\n",
      "  -2.          -1.8000002   -1.5999994   -1.3999996   -1.1999998\n",
      "  -1.          -0.8000002   -0.5999994   -0.39999962  -0.19999981\n",
      "   0.           0.19999981   0.40000057   0.6000004    0.8000002\n",
      "   1.           1.1999998    1.4000006    1.6000004    1.8000002\n",
      "   2.           2.1999998    2.4000006    2.6000004    2.8000002\n",
      "   3.           3.1999998    3.4000006    3.6000004    3.8000002\n",
      "   4.           4.2          4.4000006    4.6000004    4.8\n",
      "   5.           5.2          5.4000006    5.6000004    5.8\n",
      "   6.           6.200001     6.3999996    6.6000004    6.800001\n",
      "   7.           7.200001     7.3999996    7.6000004    7.800001\n",
      "   8.           8.200001     8.4          8.6          8.800001\n",
      "   9.           9.200001     9.4          9.6          9.800001\n",
      "  10.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "borders = model.criterion.borders.detach().cpu().numpy() \n",
    "#outputs[1] because this is the lengthscale \n",
    "plt.bar(borders[1:], torch.squeeze(outputs[1]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
