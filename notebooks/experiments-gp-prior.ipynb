{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0ac15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Kaggle. Skipping Git clone and pip install.\n",
      "Added to sys.path: /home/vdakov/Desktop/thesis/msc-thesis-vasko/src\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob - Current Token - Do not delete\n",
    "\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    print(\"Detected Kaggle environment. Starting setup...\")\n",
    "    token = getpass('Paste your GitLab token: ')\n",
    "    username = \"vdakov\" \n",
    "    repo_url = f\"https://{username}:{token}@gitlab.ewi.tudelft.nl/dsait5000/tom-viering/msc-thesis-vasko.git\"\n",
    "    !git clone {repo_url}\n",
    "    if os.path.exists(\"msc-thesis-vasko\"):\n",
    "        os.chdir(\"msc-thesis-vasko\")\n",
    "        \n",
    "    %pip install -r requirements.txt\n",
    "    if os.path.exists(\"notebooks\"):\n",
    "        os.chdir(\"notebooks\")\n",
    "\n",
    "else:\n",
    "    print(\"Not running in Kaggle. Skipping Git clone and pip install.\")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "if os.path.basename(current_dir) == 'notebooks':\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "else:\n",
    "    parent_dir = current_dir\n",
    "\n",
    "src_path = os.path.join(parent_dir, 'src')\n",
    "sys.path.append(src_path)\n",
    "print(f\"Added to sys.path: {src_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502b8f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.encoders as encoders\n",
    "from training_util import get_uniform_single_eval_pos_sampler, get_weighted_single_eval_pos_sampler, get_cosine_schedule_with_warmup\n",
    "import train\n",
    "from criterion.bar_distribution import BarDistribution, get_bucket_limits\n",
    "from models import positional_encodings\n",
    "from prior_generation import gp_prior, gp_lengthscale_prior\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "from samplers.distribution_samplers import DistributionSampler\n",
    "from samplers.distributions import ScaledBernoulli\n",
    "import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f622c4e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 57\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefinitions\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_features\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     }\n\u001b[1;32m     54\u001b[0m }\n\u001b[0;32m---> 57\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# transformer_configuration, training_configuration, criterion, generators, prior, prior_hyperparameters, context_delimiter_generator = load_config.load_config_from_yaml('../src/configs/vanilla_pfn.yaml')\u001b[39;00m\n\u001b[1;32m     59\u001b[0m transformer_configuration, training_configuration, criterion, generators, prior, prior_hyperparameters, context_delimiter_generator \u001b[38;5;241m=\u001b[39m load_config\u001b[38;5;241m.\u001b[39mparse_config_dict(config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'definitions': {\n",
    "        'num_features': 1,\n",
    "        'num_outputs': 100,\n",
    "        'sequence_length': 10,\n",
    "        'max_eval_pos': 9\n",
    "    },\n",
    "    'training_configuration': {\n",
    "        'epochs': 5,\n",
    "        'batch_size': 16,\n",
    "        'warmup_epochs': 25,\n",
    "        'steps_per_epoch': 10,\n",
    "        'validation_context_pos': 9,\n",
    "        'sequence_length': 10,\n",
    "        'lr': 0.0001,\n",
    "        'scheduler': 'cosine_scheduler',\n",
    "        'aggregate_k_gradients': 1,\n",
    "        'context_delimiter_sampling': 'uniform',\n",
    "        'context_delimiter_max_eval_pos': 9,\n",
    "        \"num_test_parameters\": 1\n",
    "    },\n",
    "    'transformer_configuration': {\n",
    "        'emsize': 512,\n",
    "        'fuse_x_y': False,\n",
    "        'nlayers': 6,\n",
    "        'num_features': 1,\n",
    "        'nhead': 4,\n",
    "        'nhid': 1024,\n",
    "        'num_outputs': 100,\n",
    "        'dropout': 0.2,\n",
    "        'input_normalization': True,\n",
    "        'encoder_type': 'linear',\n",
    "        'pos_encoder_type': 'none',\n",
    "        'y_encoder_type': 'linear'\n",
    "    },\n",
    "    'prior_configuration': {\n",
    "        'prior_learning': False,\n",
    "        'type': 'gaussian_process_prior',\n",
    "        'hyperparams': {\n",
    "            'kernel': 'rbf',\n",
    "            'length_scale': 0.5,\n",
    "            'output_scale': 1,\n",
    "            'noise_std': 0.001,\n",
    "            'num_features': 1,\n",
    "            'num_outputs': 100\n",
    "        }\n",
    "    },\n",
    "    'criterion_configuration': {\n",
    "        'loss': 'bar_distribution',\n",
    "        'min_y': -5,\n",
    "        'max_y': 5,\n",
    "        'num_buckets': 100\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# transformer_configuration, training_configuration, criterion, generators, prior, prior_hyperparameters, context_delimiter_generator = load_config.load_config_from_yaml('../src/configs/vanilla_pfn.yaml')\n",
    "transformer_configuration, training_configuration, criterion, generators, prior, prior_hyperparameters, context_delimiter_generator = load_config.parse_config_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb480b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.visualize_datasets(number_of_datasets=5, num_points_per_dataset=200, num_features_per_dataset=1, device='cpu', **prior_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, losses, positional_losses, val_losses = train.train(\n",
    "    prior_dataloader=prior,\n",
    "    criterion=criterion, # Passing the wrapper\n",
    "    transformer_configuration=transformer_configuration,\n",
    "    generators = generators,\n",
    "    training_configuration=training_configuration,\n",
    "    prior_hyperparameters=prior_hyperparameters,\n",
    "    context_delimiter_generator = context_delimiter_generator,\n",
    "    save_folder=\"../results\",\n",
    "    load_path=\"../results/test-model/checkpoint.pt\",\n",
    "    experiment_name=\"test-model\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualization.training_plots as training_plots \n",
    "training_plots.plot_training_validation_loss(losses, val_losses, title=\"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_in_dataset = 15\n",
    "num_training_points = num_points_in_dataset - 5\n",
    "num_datasets = 9\n",
    "\n",
    "train_X, train_Y, y_target, _ = prior.get_datasets_from_prior(num_datasets, num_points_in_dataset, 1, **prior_hyperparameters)\n",
    "train_X = train_X.to(device)\n",
    "train_Y = train_Y.to(device)\n",
    "y_target = y_target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75332f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.prediction_visualization import show_vanilla_pfn_predictions\n",
    "\n",
    "show_vanilla_pfn_predictions(model, train_X, train_Y, num_datasets, num_training_points, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91098735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definitions:\n",
      "  num_features: 1\n",
      "  num_outputs: 250\n",
      "  sequence_length: 10\n",
      "  max_eval_pos: 9\n",
      "training_configuration:\n",
      "  epochs: 500\n",
      "  batch_size: 256\n",
      "  warmup_epochs: 25\n",
      "  steps_per_epoch: 10\n",
      "  validation_context_pos: 10\n",
      "  sequence_length: 10\n",
      "  lr: 0.0001\n",
      "  scheduler: cosine_scheduler\n",
      "  aggregate_k_gradients: 1\n",
      "  context_delimiter_sampling: constant_last\n",
      "  context_delimiter_max_eval_pos: 9\n",
      "  num_test_parameters: 1\n",
      "transformer_configuration:\n",
      "  emsize: 512\n",
      "  fuse_x_y: false\n",
      "  nlayers: 6\n",
      "  num_features: 1\n",
      "  nhead: 4\n",
      "  nhid: 1024\n",
      "  num_outputs: 250\n",
      "  dropout: 0.2\n",
      "  input_normalization: true\n",
      "  encoder_type: linear\n",
      "  pos_encoder_type: none\n",
      "  y_encoder_type: linear\n",
      "prior_configuration:\n",
      "  prior_learning: true\n",
      "  type: gaussian_process_lengtscale_prior\n",
      "  hyperparams:\n",
      "    kernel: rbf\n",
      "    output_scale: 1\n",
      "    noise_std: 0.001\n",
      "    num_features: 1\n",
      "    num_outputs: 250\n",
      "    samplers:\n",
      "      length_scale:\n",
      "        distribution: scaled_bernoulli\n",
      "        low: 0.4\n",
      "        high: 0.6\n",
      "        p: 0.5\n",
      "criterion_configuration:\n",
      "  loss: bar_distribution\n",
      "  min_y: -5\n",
      "  max_y: 5\n",
      "  num_buckets: 250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdakov/.local/lib/python3.12/site-packages/torch/distributions/distribution.py:62: UserWarning: <class 'samplers.distributions.ScaledBernoulli'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "transformer_configuration, training_configuration, criterion, generators, prior, prior_hyperparameters, context_delimiter_generator = load_config.load_config_from_yaml('../src/configs/prior_learning_pfn.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6073d2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdakov/.local/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/vdakov/.local/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Total Number of Datasets: 1280000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss  1.65 | pos loss   nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 1.65, lr 8.000000000000001e-06 val score 1.2227098941802979:   1%|          | 3/500 [03:12<8:50:15, 64.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, losses, positional_losses, val_losses \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m      2\u001b[0m     prior_dataloader\u001b[38;5;241m=\u001b[39mprior,\n\u001b[1;32m      3\u001b[0m     criterion\u001b[38;5;241m=\u001b[39mcriterion, \u001b[38;5;66;03m# Passing the wrapper\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     transformer_configuration\u001b[38;5;241m=\u001b[39mtransformer_configuration,\n\u001b[1;32m      5\u001b[0m     generators \u001b[38;5;241m=\u001b[39m generators,\n\u001b[1;32m      6\u001b[0m     training_configuration\u001b[38;5;241m=\u001b[39mtraining_configuration,\n\u001b[1;32m      7\u001b[0m     prior_hyperparameters\u001b[38;5;241m=\u001b[39mprior_hyperparameters,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# load_path=\"../results/test-model-2/checkpoint.pt\",\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     context_delimiter_generator \u001b[38;5;241m=\u001b[39m context_delimiter_generator,\n\u001b[1;32m     10\u001b[0m     save_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     experiment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-model-2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/thesis/msc-thesis-vasko/src/train.py:119\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(prior_dataloader, criterion, transformer_configuration, generators, training_configuration, prior_hyperparameters, load_path, context_delimiter_generator, device, save_folder, experiment_name, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m--> 119\u001b[0m     loss, positional_loss \u001b[38;5;241m=\u001b[39m train_one_epoch() \n\u001b[1;32m    120\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m    121\u001b[0m     positional_losses\u001b[38;5;241m.\u001b[39mappend(positional_loss)\n",
      "File \u001b[0;32m~/Desktop/thesis/msc-thesis-vasko/src/train.py:90\u001b[0m, in \u001b[0;36mtrain.<locals>.train_one_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m losses \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39moutput\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     89\u001b[0m loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 90\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m aggregate_k_gradients \u001b[38;5;241m==\u001b[39m aggregate_k_gradients \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     92\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    649\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m _engine_run_backward(\n\u001b[1;32m    355\u001b[0m     tensors,\n\u001b[1;32m    356\u001b[0m     grad_tensors_,\n\u001b[1;32m    357\u001b[0m     retain_graph,\n\u001b[1;32m    358\u001b[0m     create_graph,\n\u001b[1;32m    359\u001b[0m     inputs_tuple,\n\u001b[1;32m    360\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    362\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/function.py:296\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[1;32m    292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    297\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, losses, positional_losses, val_losses = train.train(\n",
    "    prior_dataloader=prior,\n",
    "    criterion=criterion, # Passing the wrapper\n",
    "    transformer_configuration=transformer_configuration,\n",
    "    generators = generators,\n",
    "    training_configuration=training_configuration,\n",
    "    prior_hyperparameters=prior_hyperparameters,\n",
    "    # load_path=\"../results/test-model-2/checkpoint.pt\",\n",
    "    context_delimiter_generator = context_delimiter_generator,\n",
    "    save_folder=\"../results\",\n",
    "    experiment_name=\"test-model-2\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0424c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_in_dataset = 100\n",
    "hyperparameters = { 'kernel': \"rbf\", 'length_scale': 0.4, \"samplers\": {\"length_scale\": DistributionSampler(ScaledBernoulli(low=0.4, high=0.6, prob=1.0))}}\n",
    "prior.visualize_datasets(number_of_datasets=5, num_points_per_dataset=num_points_in_dataset, num_features_per_dataset=1, device='cpu', **hyperparameters)\n",
    "train_X, train_Y, y_target, lengthscale = prior.get_datasets_from_prior(1, num_points_in_dataset, 1, **hyperparameters)\n",
    "\n",
    "train_X = train_X.to(device)\n",
    "train_Y = train_Y.to(device)\n",
    "y_target = y_target.to(device)\n",
    "num_training_points = num_points_in_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41288866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluation.evaluate_with_context import evaluate_parameter_distributions_on_model\n",
    "\n",
    "outputs = evaluate_parameter_distributions_on_model(train_X, train_Y, model, device, num_training_points)\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a893f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def show_parameter_distributions_predictions(outputs, threshold, parameters):\n",
    "    num_distributions = len(parameters)\n",
    "    borders_all = model.criterion.borders.detach().cpu().numpy()\n",
    "    widths = np.diff(borders_all)\n",
    "    print(outputs.shape)\n",
    "    for i in range(num_distributions):\n",
    "\n",
    "        left_edges = borders_all[:-1]\n",
    "        values = torch.squeeze(outputs[i]).detach().cpu().numpy()\n",
    "        mask = values > threshold\n",
    "\n",
    "        plt.bar(left_edges[mask], values[mask], width=widths[mask], align='edge', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "        plt.xlabel(parameters[i])\n",
    "        plt.ylabel(\"Probability\")\n",
    "        plt.title(f\"Focus on Significant Factors (> {threshold})\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_parameter_distributions_predictions(outputs, 0.01, ['Lengthscale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbdc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rbf_kernel(X1, X2, lengthscale, variance=1.0):\n",
    "    dists = cdist(X1, X2, metric='sqeuclidean')\n",
    "    return variance * np.exp(-0.5 * dists / (lengthscale ** 2))\n",
    "\n",
    "\n",
    "true_lengthscale = 0.4\n",
    "X = np.random.uniform(0, 1, size=(10, 1))\n",
    "K_true = rbf_kernel(X, X, lengthscale=true_lengthscale)\n",
    "y = np.random.multivariate_normal(mean=np.zeros(len(X)), cov=K_true).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def nll_fn_robust(theta, X, y):\n",
    "    ls = np.exp(theta[0])      # Lengthscale\n",
    "    sigma_f = np.exp(theta[1]) # Signal Variance (Output scale)\n",
    "    sigma_n = np.exp(theta[2]) # Noise Variance\n",
    "    \n",
    "    n = len(y)\n",
    "    # Ensure X is 2D and y is 1D\n",
    "    X = X.reshape(-1, 1) if X.ndim == 1 else X\n",
    "    y = y.ravel()\n",
    "\n",
    "    K = sigma_f**2 * np.exp(-0.5 * cdist(X, X, 'sqeuclidean') / ls**2) + (sigma_n**2 + 1e-6) * np.eye(n)\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(K)\n",
    "        alpha = np.linalg.solve(L.T, np.linalg.solve(L, y))\n",
    "        data_fit = 0.5 * y.T @ alpha\n",
    "        complexity = np.sum(np.log(np.diag(L)))\n",
    "        constant = 0.5 * n * np.log(2 * np.pi)\n",
    "        return (data_fit + complexity + constant).item()\n",
    "    except np.linalg.LinAlgError:\n",
    "        return 1e10\n",
    "\n",
    "# Optimize Lengthscale, Signal Variance, and Noise Variance\n",
    "initial_params = [np.log(0.4), np.log(1.0), np.log(1e-3)]\n",
    "res = minimize(nll_fn_robust, x0=initial_params, args=(X, y), method='L-BFGS-B')\n",
    "\n",
    "recovered_ls = np.exp(res.x[0])\n",
    "\n",
    "print(f\"True Lengthscale: {true_lengthscale}\")\n",
    "print(f\"Recovered Lengthscale: {recovered_ls:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
