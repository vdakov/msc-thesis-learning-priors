{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0ac15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Kaggle. Skipping Git clone and pip install.\n",
      "Added to sys.path: /home/vdakov/Desktop/thesis/msc-thesis-vasko/src\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    print(\"Detected Kaggle environment. Starting setup...\")\n",
    "    token = getpass('Paste your GitLab token: ')\n",
    "    username = \"vdakov\" \n",
    "    repo_url = f\"https://{username}:{token}@gitlab.ewi.tudelft.nl/dsait5000/tom-viering/msc-thesis-vasko.git\"\n",
    "    !git clone {repo_url}\n",
    "    if os.path.exists(\"msc-thesis-vasko\"):\n",
    "        os.chdir(\"msc-thesis-vasko\")\n",
    "        \n",
    "    %pip install -r requirements.txt\n",
    "    if os.path.exists(\"notebooks\"):\n",
    "        os.chdir(\"notebooks\")\n",
    "\n",
    "else:\n",
    "    print(\"Not running in Kaggle. Skipping Git clone and pip install.\")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "if os.path.basename(current_dir) == 'notebooks':\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "else:\n",
    "    parent_dir = current_dir\n",
    "\n",
    "src_path = os.path.join(parent_dir, 'src')\n",
    "# if src_path not in sys.path:\n",
    "sys.path.append(src_path)\n",
    "print(f\"Added to sys.path: {src_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502b8f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.encoders as encoders\n",
    "from training_util import get_uniform_single_eval_pos_sampler, get_weighted_single_eval_pos_sampler, get_cosine_schedule_with_warmup\n",
    "import train\n",
    "from criterion.bar_distribution import BarDistribution, get_bucket_limits\n",
    "from models import positional_encodings\n",
    "from prior_generation import gp_prior, gp_lengthscale_prior\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "from samplers.distribution_samplers import DistributionSampler\n",
    "from samplers.distributions import ScaledBernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size =  256\n",
    "warmup_epochs = 25\n",
    "steps_per_epoch = 10\n",
    "lr = 0.0001\n",
    "sequence_length = 10\n",
    "\n",
    "print('Total Dataset Size:', batch_size * epochs * steps_per_epoch)\n",
    "\n",
    "emsize = 512\n",
    "fuse_x_y = False\n",
    "nlayers = 6\n",
    "nhead = 4\n",
    "nhid = 1024\n",
    "dropout = 0.2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_buckets = 1000\n",
    "min_y = -5\n",
    "max_y = 5\n",
    "num_features = 1\n",
    "num_outputs = 1000\n",
    "my_prior_dist = dist.Uniform(low=0.1, high=1)\n",
    "sampler = DistributionSampler(my_prior_dist)\n",
    "# prior_hyperparameters = {'num_features': num_features, 'num_outputs': num_outputs, 'device': device, 'kernel': \"rbf\", 'length_scale': 0.5}\n",
    "prior_hyperparameters = {'num_features': num_features, 'num_outputs': num_outputs, 'device': device, 'kernel': \"rbf\", 'length_scale': 0.5, \"length_scale_sampling\": sampler}\n",
    "input_normalization = True\n",
    "aggregate_k_gradients=1\n",
    "encoder_type = 'linear'  # 'linear' or 'mlp'\n",
    "y_encoder_type = 'linear'\n",
    "pos_encoder_type = 'none'  # 'sinus', 'learned', 'none'\n",
    "scheduler = get_cosine_schedule_with_warmup\n",
    "prior_prediction = False\n",
    "num_test_parameters = 1\n",
    "validation_context_pos = sequence_length - 1\n",
    "\n",
    "def get_encoder_generator(encoder):\n",
    "        if encoder == 'linear':\n",
    "            encoder_generator = encoders.LinearEncoder\n",
    "        elif encoder == 'mlp':\n",
    "            encoder_generator = encoders.MLPEncoder\n",
    "        else:\n",
    "            raise NotImplementedError(f'A {encoder} encoder is not valid.')\n",
    "        return encoder_generator\n",
    "\n",
    "encoder_generator = get_encoder_generator(encoder_type)\n",
    "y_encoder_generator = get_encoder_generator(y_encoder_type)\n",
    "\n",
    "if pos_encoder_type== 'sinus':\n",
    "    pos_encoder_generator = positional_encodings.PositionalEncoding\n",
    "elif pos_encoder_type == 'learned':\n",
    "    pos_encoder_generator = positional_encodings.LearnedPositionalEncoding\n",
    "else:\n",
    "    pos_encoder_generator = positional_encodings.NoPositionalEncoding\n",
    "    \n",
    "permutation_invariant_max_eval_pos = sequence_length - 1\n",
    "permutation_invariant_sampling = 'uniform'\n",
    "\n",
    "if permutation_invariant_max_eval_pos is not None:\n",
    "    if permutation_invariant_sampling == 'weighted':\n",
    "        get_sampler = get_weighted_single_eval_pos_sampler\n",
    "    elif permutation_invariant_sampling == 'uniform':\n",
    "        get_sampler = get_uniform_single_eval_pos_sampler\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    \n",
    "context_delimiter_generator = get_sampler(permutation_invariant_max_eval_pos)\n",
    "\n",
    "transformer_configuration = (emsize, nhead, nhid, nlayers, dropout, num_features, num_outputs, input_normalization, y_encoder_generator, sequence_length, fuse_x_y, prior_prediction, num_test_parameters) \n",
    "training_configuration = (epochs, steps_per_epoch, batch_size, sequence_length, lr, warmup_epochs, aggregate_k_gradients, scheduler, prior_prediction, validation_context_pos)\n",
    "generators = (encoder_generator, y_encoder_generator, pos_encoder_generator)\n",
    "prior = gp_prior.GaussianProcessPriorGenerator()\n",
    "# prior = gp_lengthscale_prior.GaussianProcessHyperPriorGenerator()\n",
    "print(prior.name)\n",
    "criterion = BarDistribution(borders=get_bucket_limits(num_buckets, full_range=(min_y, max_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb480b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = { 'kernel': \"rbf\", 'length_scale': 0.5}\n",
    "prior.visualize_datasets(number_of_datasets=5, num_points_per_dataset=200, num_features_per_dataset=1, device='cpu', **hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting training on {device}...\")\n",
    "losses, positional_losses, val_losses,  model = train.train(\n",
    "    prior_dataloader=prior,\n",
    "    criterion=criterion, # Passing the wrapper\n",
    "    transformer_configuration=transformer_configuration,\n",
    "    generators = generators,\n",
    "    training_configuration=training_configuration,\n",
    "    prior_hyperparameters=prior_hyperparameters,\n",
    "    load_path=None,\n",
    "    context_delimiter_generator = context_delimiter_generator,\n",
    "    save_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.lineplot(x=np.arange(0, len(losses)), y=np.array(losses), label=\"Training\")\n",
    "sns.lineplot(x=np.arange(0, len(losses)), y=np.array(val_losses), label=\"Validation\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_in_dataset = 15\n",
    "train_X, train_Y, y_target = prior.get_datasets_from_prior(9, num_points_in_dataset, 1, **hyperparameters)\n",
    "train_X = train_X.to(device)\n",
    "train_Y = train_Y.to(device)\n",
    "y_target = y_target.to(device)\n",
    "num_training_points = num_points_in_dataset - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75332f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(device)\n",
    "# Set up grid for subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 8)) \n",
    "axes = axes.flatten()\n",
    "\n",
    "for batch_index in range(9):\n",
    "    ax = axes[batch_index] \n",
    "    train_x = train_X[:num_training_points, batch_index, :]\n",
    "    train_y = train_Y[:num_training_points, batch_index]\n",
    "    test_x = train_X[:, batch_index, :]\n",
    "    with torch.no_grad():\n",
    "        logits = model((torch.cat((train_x, test_x)), torch.cat((train_y, torch.zeros(len(test_x), device=device)))), context_pos=num_training_points - 1)\n",
    "\n",
    "        pred_means = model.criterion.mean(logits)\n",
    "        pred_confs = model.criterion.quantile(logits)\n",
    "        pred_means = pred_means[-len(test_x):]\n",
    "        pred_confs = pred_confs[-len(test_x):]\n",
    "        # Plot scatter points for training data\n",
    "        ax.scatter(train_x[..., 0].cpu().numpy(), train_y.cpu().numpy(), label=\"Training Data\")\n",
    "\n",
    "    # Plot model predictions\n",
    "    order_test_x = test_x[:, 0].cpu().argsort()\n",
    "    ax.plot(\n",
    "        test_x[order_test_x, 0].cpu().numpy(),\n",
    "        pred_means[order_test_x].cpu().numpy(),\n",
    "        color='green',\n",
    "        label='pfn'\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        test_x[order_test_x, 0].cpu().numpy(),\n",
    "        pred_confs[order_test_x][:, 0].cpu().numpy(),\n",
    "        pred_confs[order_test_x][:, 1].cpu().numpy(),\n",
    "        alpha=.1,\n",
    "        color='green'\n",
    "    )\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6073d2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdakov/.conda/envs/thesis/lib/python3.14/site-packages/torch/distributions/distribution.py:62: UserWarning: <class 'samplers.distributions.ScaledBernoulli'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import models.encoders as encoders\n",
    "from training_util import get_uniform_single_eval_pos_sampler, get_weighted_single_eval_pos_sampler, get_cosine_schedule_with_warmup\n",
    "import train\n",
    "from criterion.bar_distribution import BarDistribution, get_bucket_limits\n",
    "from models import positional_encodings\n",
    "from prior_generation import gp_prior, gp_lengthscale_prior\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "epochs = 500\n",
    "batch_size =  256\n",
    "warmup_epochs = 25\n",
    "steps_per_epoch = 10\n",
    "lr = 0.0001\n",
    "sequence_length = 25\n",
    "emsize = 512\n",
    "fuse_x_y = False\n",
    "nlayers = 6\n",
    "nhead = 4\n",
    "nhid = 1024\n",
    "dropout = 0.2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_buckets = 5\n",
    "min_y = 0\n",
    "max_y = 1\n",
    "num_features = 1\n",
    "num_outputs = 5\n",
    "my_prior_dist = ScaledBernoulli(0.4, 0.6)\n",
    "sampler = DistributionSampler(my_prior_dist)\n",
    "prior_hyperparameters = {'num_features': num_features, 'num_outputs': num_outputs, 'device': device, 'kernel': \"rbf\", \"length_scale_sampling\": sampler}\n",
    "input_normalization = True\n",
    "aggregate_k_gradients=1\n",
    "encoder_type = 'linear'  # 'linear' or 'mlp'\n",
    "y_encoder_type = 'linear'\n",
    "pos_encoder_type = 'none'  # 'sinus', 'learned', 'none'\n",
    "scheduler = get_cosine_schedule_with_warmup\n",
    "prior_prediction = True\n",
    "num_test_parameters = 1\n",
    "validation_context_pos = sequence_length\n",
    "\n",
    "def get_encoder_generator(encoder):\n",
    "        if encoder == 'linear':\n",
    "            encoder_generator = encoders.LinearEncoder\n",
    "        elif encoder == 'mlp':\n",
    "            encoder_generator = encoders.MLPEncoder\n",
    "        else:\n",
    "            raise NotImplementedError(f'A {encoder} encoder is not valid.')\n",
    "        return encoder_generator\n",
    "\n",
    "encoder_generator = get_encoder_generator(encoder_type)\n",
    "y_encoder_generator = get_encoder_generator(y_encoder_type)\n",
    "\n",
    "if pos_encoder_type== 'sinus':\n",
    "    pos_encoder_generator = positional_encodings.PositionalEncoding\n",
    "elif pos_encoder_type == 'learned':\n",
    "    pos_encoder_generator = positional_encodings.LearnedPositionalEncoding\n",
    "else:\n",
    "    pos_encoder_generator = positional_encodings.NoPositionalEncoding\n",
    "    \n",
    "permutation_invariant_max_eval_pos = sequence_length - 1\n",
    "permutation_invariant_sampling = 'uniform'\n",
    "\n",
    "if permutation_invariant_max_eval_pos is not None:\n",
    "    if permutation_invariant_sampling == 'weighted':\n",
    "        get_sampler = get_weighted_single_eval_pos_sampler\n",
    "    elif permutation_invariant_sampling == 'uniform':\n",
    "        get_sampler = get_uniform_single_eval_pos_sampler\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    \n",
    "context_delimiter_generator = get_sampler(permutation_invariant_max_eval_pos)\n",
    "context_delimiter_generator = lambda : sequence_length\n",
    "\n",
    "transformer_configuration = (emsize, nhead, nhid, nlayers, dropout, num_features, num_outputs, input_normalization, y_encoder_generator, sequence_length, fuse_x_y, prior_prediction, num_test_parameters) \n",
    "training_configuration = (epochs, steps_per_epoch, batch_size, sequence_length, lr, warmup_epochs, aggregate_k_gradients, scheduler, prior_prediction, validation_context_pos)\n",
    "generators = (encoder_generator, y_encoder_generator, pos_encoder_generator)\n",
    "# prior = gp_prior.GaussianProcessPriorGenerator()\n",
    "prior = gp_lengthscale_prior.GaussianProcessHyperPriorGenerator()\n",
    "criterion = BarDistribution(borders=get_bucket_limits(num_buckets, full_range=(min_y, max_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52042e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu...\n",
      "Using cpu:0 device\n",
      "1\n",
      "1\n",
      "Total Number of Datasets: 1280000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss -0.00 | pos loss   nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,-0.00, lr 0.0 val score nan:   0%|          | 1/500 [01:15<10:28:03, 75.52s/it]"
     ]
    }
   ],
   "source": [
    "# 6. Run Training\n",
    "print(f\"Starting training on {device}...\")\n",
    "losses, positional_losses, val_losses,  model = train.train(\n",
    "    prior_dataloader=prior,\n",
    "    criterion=criterion, # Passing the wrapper\n",
    "    transformer_configuration=transformer_configuration,\n",
    "    generators = generators,\n",
    "    training_configuration=training_configuration,\n",
    "    prior_hyperparameters=prior_hyperparameters,\n",
    "    load_path=None,\n",
    "    context_delimiter_generator = context_delimiter_generator,\n",
    "    device=device,\n",
    "    save_path=None,\n",
    ")\n",
    "# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0424c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_in_dataset = 100\n",
    "hyperparameters = { 'kernel': \"rbf\", 'length_scale': 0.4, \"length_scale_sampling\": DistributionSampler(dist.Uniform(low=0.4, high=0.401))}\n",
    "prior.visualize_datasets(number_of_datasets=5, num_points_per_dataset=num_points_in_dataset, num_features_per_dataset=1, device='cpu', **hyperparameters)\n",
    "train_X, train_Y, y_target, lengthscale = prior.get_datasets_from_prior(1, num_points_in_dataset, 1, **hyperparameters)\n",
    "print(lengthscale)\n",
    "train_X = train_X.to(device)\n",
    "train_Y = train_Y.to(device)\n",
    "y_target = y_target.to(device)\n",
    "num_training_points = num_points_in_dataset\n",
    "\n",
    "import seaborn as sns \n",
    "model = model.to(device)\n",
    "train_x = train_X\n",
    "train_y = train_Y\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model((train_x, train_y), context_pos=num_training_points)\n",
    "    outputs = torch.exp(torch.log_softmax(logits, -1))\n",
    "outputs = torch.squeeze(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Get all borders first (don't slice yet)\n",
    "borders_all = model.criterion.borders.detach().cpu().numpy()\n",
    "\n",
    "# 2. Calculate the width of each bin\n",
    "widths = np.diff(borders_all)\n",
    "\n",
    "# 3. Define X coordinates (using the left edge of each bin is safer for alignment)\n",
    "left_edges = borders_all[:-1]\n",
    "\n",
    "# 4. Get Values and Mask\n",
    "values = torch.squeeze(outputs).detach().cpu().numpy()\n",
    "threshold = 0.001\n",
    "mask = values > threshold\n",
    "\n",
    "# 5. Plot with specific widths and 'edge' alignment\n",
    "# align='edge' means the bar starts at x and extends to the right by 'width'\n",
    "plt.bar(left_edges[mask], values[mask], width=widths[mask], align='edge', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel(\"Lengthscale\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(f\"Focus on Significant Factors (> {threshold})\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
