{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ac15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Securely input your token\n",
    "token = getpass('Paste your GitLab token: ')\n",
    "\n",
    "# Set the repository URL\n",
    "# repo_url = \"https://oauth2:\" + token + \"@gitlab.ewi.tudelft.nl/dsait5000/tom-viering/msc-thesis-vasko.git\"\n",
    "username = \"vdakov\" \n",
    "\n",
    "# Syntax: https://<username>:<token>@<domain>/...\n",
    "repo_url = f\"https://{username}:{token}@gitlab.ewi.tudelft.nl/dsait5000/tom-viering/msc-thesis-vasko.git\"\n",
    "\n",
    "!git clone {repo_url}\n",
    "\n",
    "# Clone the repository\n",
    "# !git clone {repo_url}\n",
    "# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob\n",
    "# Verify clone\n",
    "os.chdir(\"msc-thesis-vasko\")\n",
    "%pip install -r requirements.txt\n",
    "os.chdir(\"notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c539e1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: /home/vdakov/Desktop/thesis/msc-thesis-vasko\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(os.path.join(parent_dir, 'src'))\n",
    "print(f\"Added to sys.path: {parent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f689e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Distribution\n",
    "\n",
    "class DistributionSampler:\n",
    "    def __init__(self, distribution: Distribution):\n",
    "        \"\"\"\n",
    "        :param distribution: An instantiated torch.distributions object \n",
    "                             (e.g., Uniform(0.1, 3.0), Gamma(2.0, 2.0))\n",
    "        \"\"\"\n",
    "        self.distribution = distribution\n",
    "\n",
    "    def sample(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the distribution with the given batch size.\n",
    "        Returns a tensor of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # PyTorch distributions expect a tuple/torch.Size for sampling\n",
    "        return self.distribution.sample(torch.Size([batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f622c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Process with HyperPrior\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import models.encoders as encoders\n",
    "from training_util import get_uniform_single_eval_pos_sampler, get_weighted_single_eval_pos_sampler, get_cosine_schedule_with_warmup\n",
    "import train\n",
    "from criterion.bar_distribution import BarDistribution, get_bucket_limits\n",
    "from models import positional_encodings\n",
    "from prior_generation import gp_prior, gp_lengthscale_prior\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "epochs = 2000\n",
    "batch_size =  256\n",
    "warmup_epochs = 25\n",
    "steps_per_epoch = 10\n",
    "lr = 0.0001\n",
    "sequence_length = 10\n",
    "emsize = 512\n",
    "fuse_x_y = False\n",
    "nlayers = 6\n",
    "nhead = 4\n",
    "nhid = 1024\n",
    "dropout = 0.2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_buckets = 100\n",
    "min_y = -10\n",
    "max_y = 10\n",
    "num_features = 1\n",
    "num_outputs = 100\n",
    "my_prior_dist = dist.Uniform(low=0.1, high=1)\n",
    "\n",
    "sampler = DistributionSampler(my_prior_dist)\n",
    "# prior_hyperparameters = {'num_features': num_features, 'num_outputs': num_outputs, 'device': device, 'kernel': \"rbf\", 'length_scale': 0.5}\n",
    "prior_hyperparameters = {'num_features': num_features, 'num_outputs': num_outputs, 'device': device, 'kernel': \"rbf\", 'length_scale': 0.5, \"length_scale_sampling\": sampler}\n",
    "input_normalization = True\n",
    "aggregate_k_gradients=1\n",
    "encoder_type = 'linear'  # 'linear' or 'mlp'\n",
    "y_encoder_type = 'linear'\n",
    "pos_encoder_type = 'none'  # 'sinus', 'learned', 'none'\n",
    "scheduler = get_cosine_schedule_with_warmup\n",
    "prior_prediction = True\n",
    "\n",
    "def get_encoder_generator(encoder):\n",
    "        if encoder == 'linear':\n",
    "            encoder_generator = encoders.LinearEncoder\n",
    "        elif encoder == 'mlp':\n",
    "            encoder_generator = encoders.MLPEncoder\n",
    "        else:\n",
    "            raise NotImplementedError(f'A {encoder} encoder is not valid.')\n",
    "        return encoder_generator\n",
    "\n",
    "encoder_generator = get_encoder_generator(encoder_type)\n",
    "y_encoder_generator = get_encoder_generator(y_encoder_type)\n",
    "\n",
    "if pos_encoder_type== 'sinus':\n",
    "    pos_encoder_generator = positional_encodings.PositionalEncoding\n",
    "elif pos_encoder_type == 'learned':\n",
    "    pos_encoder_generator = positional_encodings.LearnedPositionalEncoding\n",
    "else:\n",
    "    pos_encoder_generator = positional_encodings.NoPositionalEncoding\n",
    "    \n",
    "permutation_invariant_max_eval_pos = sequence_length - 1\n",
    "permutation_invariant_sampling = 'uniform'\n",
    "\n",
    "if permutation_invariant_max_eval_pos is not None:\n",
    "    if permutation_invariant_sampling == 'weighted':\n",
    "        get_sampler = get_weighted_single_eval_pos_sampler\n",
    "    elif permutation_invariant_sampling == 'uniform':\n",
    "        get_sampler = get_uniform_single_eval_pos_sampler\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    \n",
    "context_delimiter_generator = get_sampler(permutation_invariant_max_eval_pos)\n",
    "\n",
    "transformer_configuration = (emsize, nhead, nhid, nlayers, dropout, num_features, num_outputs, input_normalization, y_encoder_generator, sequence_length, fuse_x_y, prior_prediction) \n",
    "training_configuration = (epochs, steps_per_epoch, batch_size, sequence_length, lr, warmup_epochs, aggregate_k_gradients, scheduler, prior_prediction)\n",
    "generators = (encoder_generator, y_encoder_generator, pos_encoder_generator)\n",
    "# prior = gp_prior.GaussianProcessPriorGenerator()\n",
    "prior = gp_lengthscale_prior.GaussianProcessHyperPriorGenerator()\n",
    "print(prior.name)\n",
    "criterion = BarDistribution(borders=get_bucket_limits(num_buckets, full_range=(min_y, max_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb480b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prior' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m hyperparameters = { \u001b[33m'\u001b[39m\u001b[33mkernel\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mrbf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlength_scale\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.1\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mprior\u001b[49m.visualize_datasets(number_of_datasets=\u001b[32m5\u001b[39m, num_points_per_dataset=\u001b[32m200\u001b[39m, num_features_per_dataset=\u001b[32m1\u001b[39m, device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m, **hyperparameters)\n",
      "\u001b[31mNameError\u001b[39m: name 'prior' is not defined"
     ]
    }
   ],
   "source": [
    "hyperparameters = { 'kernel': \"rbf\", 'length_scale': 0.1}\n",
    "prior.visualize_datasets(number_of_datasets=5, num_points_per_dataset=200, num_features_per_dataset=1, device='cpu', **hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d125b87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu...\n",
      "Using cpu:0 device\n",
      "Dataset.__dict__ {'num_steps': 10, 'fuse_x_y': False, 'prior_prediction': True, 'get_batch_kwargs': {'batch_size': 256, 'seq_len': 10, 'num_features': 1, 'num_outputs': 100, 'device': 'cpu', 'kernel': 'rbf', 'length_scale': 0.5, 'length_scale_sampling': <__main__.DistributionSampler object at 0x7119eedb2ba0>}, 'num_features': 1, 'num_outputs': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdakov/.conda/envs/thesis/lib/python3.14/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/vdakov/.conda/envs/thesis/lib/python3.14/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader.__dict__ {'num_steps': 10, 'fuse_x_y': False, 'get_batch_kwargs': {'batch_size': 256, 'seq_len': 10, 'prior_prediction': True, 'num_features': 1, 'num_outputs': 100, 'device': 'cpu', 'kernel': 'rbf', 'length_scale': 0.5, 'length_scale_sampling': <__main__.DistributionSampler object at 0x7119eedb2ba0>}, 'PriorDataset': <class 'prior_generation.prior_dataloader.get_dataloader.<locals>.PriorDataset'>, 'num_features': 1, 'num_outputs': 100, 'validation_set': ((tensor([[[0.2965],\n",
      "         [0.5819],\n",
      "         [0.2140],\n",
      "         ...,\n",
      "         [0.4071],\n",
      "         [0.4650],\n",
      "         [0.3600]],\n",
      "\n",
      "        [[0.5402],\n",
      "         [0.4846],\n",
      "         [0.2010],\n",
      "         ...,\n",
      "         [0.0886],\n",
      "         [0.6546],\n",
      "         [0.5766]],\n",
      "\n",
      "        [[0.0409],\n",
      "         [0.0458],\n",
      "         [0.9787],\n",
      "         ...,\n",
      "         [0.8188],\n",
      "         [0.5143],\n",
      "         [0.6662]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3598],\n",
      "         [0.9634],\n",
      "         [0.9171],\n",
      "         ...,\n",
      "         [0.6523],\n",
      "         [0.7977],\n",
      "         [0.8432]],\n",
      "\n",
      "        [[0.1949],\n",
      "         [0.9218],\n",
      "         [0.3024],\n",
      "         ...,\n",
      "         [0.5272],\n",
      "         [0.1159],\n",
      "         [0.5205]],\n",
      "\n",
      "        [[0.9875],\n",
      "         [0.2645],\n",
      "         [0.7891],\n",
      "         ...,\n",
      "         [0.9237],\n",
      "         [0.5454],\n",
      "         [0.6598]]]), tensor([[ 1.0901, -0.5512,  0.8696,  ...,  1.2479, -1.1409, -0.6190],\n",
      "        [ 1.4670, -0.3657,  0.8598,  ...,  1.6853, -0.3900, -0.7707],\n",
      "        [ 0.7658,  0.6618,  1.0968,  ...,  0.4101, -0.6985, -0.8057],\n",
      "        ...,\n",
      "        [ 1.1851, -0.7992,  1.1116,  ...,  0.8839, -0.9090, -0.8593],\n",
      "        [ 0.9471, -0.8058,  0.9380,  ...,  1.1048,  0.3671, -0.7418],\n",
      "        [ 1.9113,  0.1572,  1.1208,  ...,  0.0382, -0.5598, -0.8038]],\n",
      "       grad_fn=<TransposeBackward0>)), tensor([[ 1.0901, -0.5512,  0.8696,  ...,  1.2479, -1.1409, -0.6190],\n",
      "        [ 1.4670, -0.3657,  0.8598,  ...,  1.6853, -0.3900, -0.7707],\n",
      "        [ 0.7658,  0.6618,  1.0968,  ...,  0.4101, -0.6985, -0.8057],\n",
      "        ...,\n",
      "        [ 1.1851, -0.7992,  1.1116,  ...,  0.8839, -0.9090, -0.8593],\n",
      "        [ 0.9471, -0.8058,  0.9380,  ...,  1.1048,  0.3671, -0.7418],\n",
      "        [ 1.9113,  0.1572,  1.1208,  ...,  0.0382, -0.5598, -0.8038]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[0.7015, 0.6317, 0.7247, 0.6266, 0.2289, 0.9862, 0.3470, 0.3070, 0.3365,\n",
      "         0.1711, 0.7794, 0.6457, 0.3387, 0.5800, 0.6120, 0.9115, 0.8523, 0.9911,\n",
      "         0.7374, 0.7958, 0.1441, 0.1587, 0.6002, 0.1519, 0.4970, 0.3215, 0.3837,\n",
      "         0.3043, 0.1003, 0.1968, 0.9237, 0.8333, 0.6825, 0.3969, 0.6555, 0.8770,\n",
      "         0.6470, 0.1533, 0.1733, 0.8649, 0.7438, 0.3807, 0.9190, 0.2155, 0.7613,\n",
      "         0.9708, 0.8004, 0.3084, 0.3681, 0.6395, 0.6149, 0.3481, 0.2437, 0.9922,\n",
      "         0.7882, 0.8567, 0.9297, 0.1067, 0.7007, 0.5882, 0.3550, 0.4246, 0.9602,\n",
      "         0.4359, 0.2291, 0.3699, 0.6670, 0.3757, 0.8628, 0.7062, 0.5110, 0.6988,\n",
      "         0.6868, 0.9627, 0.6210, 0.6805, 0.7301, 0.3256, 0.4797, 0.7957, 0.8796,\n",
      "         0.4229, 0.4751, 0.6827, 0.7183, 0.5329, 0.7558, 0.5360, 0.3910, 0.7688,\n",
      "         0.9833, 0.5715, 0.2786, 0.4287, 0.9203, 0.6507, 0.7994, 0.6687, 0.1250,\n",
      "         0.8475, 0.5944, 0.4600, 0.4786, 0.3724, 0.8926, 0.4440, 0.4907, 0.9699,\n",
      "         0.9120, 0.7652, 0.7360, 0.9883, 0.6402, 0.2703, 0.7746, 0.9211, 0.7283,\n",
      "         0.6533, 0.3910, 0.7589, 0.7292, 0.4477, 0.7084, 0.1487, 0.6646, 0.7910,\n",
      "         0.4282, 0.2450, 0.3730, 0.2123, 0.8556, 0.2590, 0.6560, 0.1889, 0.6423,\n",
      "         0.6920, 0.3108, 0.2950, 0.3795, 0.4911, 0.7823, 0.3809, 0.8693, 0.6731,\n",
      "         0.7333, 0.8784, 0.5185, 0.5367, 0.3702, 0.6940, 0.7043, 0.2238, 0.4982,\n",
      "         0.3795, 0.3717, 0.6347, 0.8021, 0.6219, 0.5066, 0.4598, 0.1797, 0.3279,\n",
      "         0.7376, 0.9024, 0.7274, 0.1737, 0.9302, 0.9631, 0.9287, 0.4811, 0.2431,\n",
      "         0.7519, 0.7078, 0.3223, 0.1998, 0.5551, 0.1419, 0.4503, 0.2612, 0.4486,\n",
      "         0.9792, 0.1836, 0.1217, 0.7796, 0.2557, 0.7543, 0.4740, 0.6195, 0.3154,\n",
      "         0.8786, 0.1890, 0.2479, 0.3112, 0.4846, 0.3758, 0.8886, 0.1122, 0.1285,\n",
      "         0.8194, 0.3973, 0.7141, 0.2188, 0.4907, 0.6427, 0.2260, 0.5107, 0.7720,\n",
      "         0.6171, 0.2218, 0.5868, 0.4550, 0.1307, 0.8665, 0.2672, 0.9281, 0.1945,\n",
      "         0.9591, 0.5146, 0.3402, 0.2742, 0.1091, 0.4542, 0.2598, 0.5654, 0.6257,\n",
      "         0.2327, 0.7723, 0.1051, 0.5003, 0.1756, 0.3691, 0.4813, 0.3051, 0.6485,\n",
      "         0.2089, 0.1733, 0.9155, 0.9488, 0.8505, 0.5010, 0.2691, 0.6195, 0.8048,\n",
      "         0.1787, 0.6021, 0.1692, 0.4456, 0.2919, 0.9595, 0.9127, 0.5671, 0.2280,\n",
      "         0.3964, 0.3465, 0.1050, 0.5929]])), 'context_position_val': 6, 'dataset': <prior_generation.prior_dataloader.get_dataloader.<locals>.PriorDataset object at 0x7119eedb3230>, 'num_workers': 0, 'prefetch_factor': None, 'pin_memory': False, 'pin_memory_device': '', 'timeout': 0, 'worker_init_fn': None, '_DataLoader__multiprocessing_context': None, 'in_order': True, '_dataset_kind': 1, 'batch_size': None, 'drop_last': False, 'sampler': <torch.utils.data.dataloader._InfiniteConstantSampler object at 0x7119eedb3e00>, 'batch_sampler': None, 'generator': None, 'collate_fn': <function default_convert at 0x711a4c9862a0>, 'persistent_workers': False, '_DataLoader__initialized': True, '_IterableDataset_len_called': None, '_iterator': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2000 [03:22<18:41:23, 33.74s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 6. Run Training\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting training on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m losses, positional_losses, val_losses,  model = \u001b[43mtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprior_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Passing the wrapper\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformer_configuration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerators\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_configuration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprior_hyperparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprior_hyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_delimiter_generator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_delimiter_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/thesis/msc-thesis-vasko/src/train.py:108\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(prior_dataloader, criterion, transformer_configuration, generators, training_configuration, prior_hyperparameters, load_path, context_delimiter_generator, device, verbose, save_path, **kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m losses, positional_losses, val_losses = [], [], []\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     loss, positional_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    109\u001b[39m     losses.append(loss)\n\u001b[32m    110\u001b[39m     positional_losses.append(positional_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/thesis/msc-thesis-vasko/src/train.py:88\u001b[39m, in \u001b[36mtrain.<locals>.train_one_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     86\u001b[39m losses = losses.view(*output.shape[\u001b[32m0\u001b[39m:\u001b[32m2\u001b[39m]).squeeze(-\u001b[32m1\u001b[39m)\n\u001b[32m     87\u001b[39m loss = losses.mean()\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch % aggregate_k_gradients == aggregate_k_gradients - \u001b[32m1\u001b[39m:\n\u001b[32m     90\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/thesis/lib/python3.14/site-packages/torch/autograd/function.py:300\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBackwardCFunction\u001b[39;00m(_C._FunctionBase, FunctionCtx, _HookMixin):\n\u001b[32m    296\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[33;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m    301\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[33;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m    304\u001b[39m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[32m    305\u001b[39m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 6. Run Training\n",
    "print(f\"Starting training on {device}...\")\n",
    "losses, positional_losses, val_losses,  model = train.train(\n",
    "    prior_dataloader=prior,\n",
    "    criterion=criterion, # Passing the wrapper\n",
    "    transformer_configuration=transformer_configuration,\n",
    "    generators = generators,\n",
    "    training_configuration=training_configuration,\n",
    "    prior_hyperparameters=prior_hyperparameters,\n",
    "    load_path=None,\n",
    "    context_delimiter_generator = context_delimiter_generator,\n",
    "    device=device,\n",
    "    verbose=False,\n",
    "    save_path=None,\n",
    ")\n",
    "# -7tPNnta8dSg6H33ZZcSz286MQp1OjhjeAk.01.0z1wd0yob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.lineplot(x=np.arange(0, len(losses)), y=np.array(losses), label=\"Training\")\n",
    "sns.lineplot(x=np.arange(0, len(losses)), y=np.array(val_losses), label=\"Validation\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_in_dataset = 15\n",
    "train_X, train_Y, y_target = prior.get_datasets_from_prior(9, num_points_in_dataset, 1, **hyperparameters)\n",
    "train_X = train_X.to(device)\n",
    "train_Y = train_Y.to(device)\n",
    "y_target = y_target.to(device)\n",
    "num_training_points = num_points_in_dataset - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75332f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(device)\n",
    "# Set up grid for subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 8)) \n",
    "axes = axes.flatten()\n",
    "\n",
    "for batch_index in range(9):\n",
    "    ax = axes[batch_index] \n",
    "    train_x = train_X[:num_training_points, batch_index, :]\n",
    "    train_y = train_Y[:num_training_points, batch_index]\n",
    "    test_x = train_X[:, batch_index, :]\n",
    "    with torch.no_grad():\n",
    "        logits = model((torch.cat((train_x, test_x)), torch.cat((train_y, torch.zeros(len(test_x), device=device)))), context_pos=num_training_points - 1)\n",
    "\n",
    "        pred_means = model.criterion.mean(logits)\n",
    "        pred_confs = model.criterion.quantile(logits)\n",
    "        pred_means = pred_means[-len(test_x):]\n",
    "        pred_confs = pred_confs[-len(test_x):]\n",
    "        # Plot scatter points for training data\n",
    "        ax.scatter(train_x[..., 0].cpu().numpy(), train_y.cpu().numpy(), label=\"Training Data\")\n",
    "\n",
    "    # Plot model predictions\n",
    "    order_test_x = test_x[:, 0].cpu().argsort()\n",
    "    print(test_x.shape)\n",
    "    print(pred_means.shape)\n",
    "    print(pred_confs.shape)\n",
    "    ax.plot(\n",
    "        test_x[order_test_x, 0].cpu().numpy(),\n",
    "        pred_means[order_test_x].cpu().numpy(),\n",
    "        color='green',\n",
    "        label='pfn'\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        test_x[order_test_x, 0].cpu().numpy(),\n",
    "        pred_confs[order_test_x][:, 0].cpu().numpy(),\n",
    "        pred_confs[order_test_x][:, 1].cpu().numpy(),\n",
    "        alpha=.1,\n",
    "        color='green'\n",
    "    )\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
