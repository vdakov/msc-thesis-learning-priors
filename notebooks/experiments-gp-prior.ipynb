{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c539e1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: /home/vdakov/Desktop/thesis/msc-thesis-vasko\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(os.path.join(parent_dir, 'src'))\n",
    "print(f\"Added to sys.path: {parent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f622c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu...\n",
      "Using cpu:0 device\n",
      "{'num_features': 1, 'num_outputs': 100, 'device': 'cpu'}\n",
      "Dataset.__dict__ {'num_steps': 50, 'fuse_x_y': False, 'get_batch_kwargs': {'batch_size': 100, 'seq_len': 10, 'num_features': 1, 'num_outputs': 100, 'device': 'cpu'}, 'num_features': 1, 'num_outputs': 100}\n",
      "DataLoader.__dict__ {'num_steps': 50, 'fuse_x_y': False, 'get_batch_kwargs': {'batch_size': 100, 'seq_len': 10, 'num_features': 1, 'num_outputs': 100, 'device': 'cpu'}, 'PriorDataset': <class 'prior_generation.prior_dataloader.get_dataloader.<locals>.PriorDataset'>, 'num_features': 1, 'num_outputs': 100, 'dataset': <prior_generation.prior_dataloader.get_dataloader.<locals>.PriorDataset object at 0x70028e6fe510>, 'num_workers': 0, 'prefetch_factor': None, 'pin_memory': False, 'pin_memory_device': '', 'timeout': 0, 'worker_init_fn': None, '_DataLoader__multiprocessing_context': None, 'in_order': True, '_dataset_kind': 1, 'batch_size': None, 'drop_last': False, 'sampler': <torch.utils.data.dataloader._InfiniteConstantSampler object at 0x70028e4965d0>, 'batch_sampler': None, 'generator': None, 'collate_fn': <function default_convert at 0x7003f42bf060>, 'persistent_workers': False, '_DataLoader__initialized': True, '_IterableDataset_len_called': None, '_iterator': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdakov/.conda/envs/thesis/lib/python3.14/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/home/vdakov/.conda/envs/thesis/lib/python3.14/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  1.93s | mean loss  0.11 | pos losses  5.55, 5.54, 5.45, 5.41, 5.39, 5.33, 5.28, 5.31, 5.36, 5.37, lr 0.0 data time  0.04 step time  1.89 forward time  0.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  1.42s | mean loss  0.11 | pos losses  5.57, 5.56, 5.45, 5.41, 5.38, 5.33, 5.27, 5.32, 5.38, 5.39, lr 0.0001 data time  0.01 step time  1.40 forward time  0.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  1.57s | mean loss  0.09 | pos losses  4.69, 4.66, 4.58, 4.54, 4.54, 4.50, 4.45, 4.49, 4.53, 4.56, lr 0.0002 data time  0.01 step time  1.55 forward time  0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  1.40s | mean loss  0.07 | pos losses  3.44, 3.34, 3.29, 3.29, 3.32, 3.27, 3.27, 3.33, 3.30, 3.34, lr 0.0003 data time  0.01 step time  1.37 forward time  0.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  1.47s | mean loss  0.05 | pos losses  2.58, 2.47, 2.51, 2.57, 2.51, 2.49, 2.50, 2.48, 2.48, 2.47, lr 0.0004 data time  0.01 step time  1.44 forward time  0.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  1.37s | mean loss  0.04 | pos losses  2.07, 1.92, 1.87, 1.97, 1.94, 1.88, 1.96, 2.03, 1.91, 1.89, lr 0.0005 data time  0.01 step time  1.34 forward time  0.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  1.32s | mean loss  0.03 | pos losses  1.83, 1.61, 1.46, 1.54, 1.52, 1.46, 1.55, 1.46, 1.58, 1.53, lr 0.0006 data time  0.01 step time  1.30 forward time  0.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  1.30s | mean loss  0.02 | pos losses  1.49, 1.10, 1.04, 1.09, 0.99, 1.04, 1.04, 1.01, 1.03, 1.00, lr 0.0007 data time  0.01 step time  1.28 forward time  0.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  1.49s | mean loss  0.04 | pos losses  3.83, 1.96, 1.69, 1.94, 1.76, 2.25, 2.05, 1.96, 1.84, 1.61, lr 0.0008 data time  0.01 step time  1.47 forward time  0.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  1.29s | mean loss  0.02 | pos losses  2.33, 1.10, 1.16, 1.03, 0.92, 0.99, 1.05, 0.98, 1.09, 1.13, lr 0.0009000000000000001 data time  0.01 step time  1.26 forward time  0.42\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import models.encoders as encoders\n",
    "import train\n",
    "from criterion.bar_distribution import BarDistribution, get_bucket_limits\n",
    "from models import positional_encodings\n",
    "from prior_generation import gp_prior\n",
    "import torch\n",
    "\n",
    "\n",
    "args = {\n",
    "    'epochs': 10,\n",
    "    'batch_size': 100,\n",
    "    'steps_per_epoch': 50,\n",
    "    'lr': 0.001,\n",
    "    'sequence_length': 10,\n",
    "    'emsize': 512,\n",
    "    'nlayers': 6,\n",
    "    'nhead': 4,\n",
    "    'nhid': 1024,\n",
    "    'dropout': 0.0,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Task specific\n",
    "    'num_buckets': 100,\n",
    "    'min_y': -100.0,\n",
    "    'max_y': 100.0,\n",
    "    'prior_hyperparameters': {'num_features': 1, 'num_outputs': 100, 'device': 'cpu'},\n",
    "    \n",
    "    # Encoders\n",
    "    'input_normalization': False,\n",
    "    'encoder_type': 'linear', # 'linear' or 'mlp'\n",
    "    'pos_encoder_type': 'sinus' # 'sinus', 'learned', 'none'\n",
    "}\n",
    "\n",
    "# 5. Setup Components (Mimicking the logic in your main block)\n",
    "# Prior\n",
    "prior = gp_prior.GaussianProcessPriorGenerator()\n",
    "criterion = BarDistribution(borders=get_bucket_limits(args['num_buckets'], full_range=(args['min_y'], args['max_y'])))\n",
    "\n",
    "# Encoders\n",
    "if args['encoder_type'] == 'linear':\n",
    "    encoder_generator = encoders.LinearEncoder\n",
    "else:\n",
    "    encoder_generator = encoders.MLPEncoder\n",
    "\n",
    "if args['pos_encoder_type'] == 'sinus':\n",
    "    pos_encoder_generator = positional_encodings.PositionalEncoding\n",
    "elif args['pos_encoder_type'] == 'learned':\n",
    "    pos_encoder_generator = positional_encodings.LearnedPositionalEncoding\n",
    "else:\n",
    "    pos_encoder_generator = positional_encodings.NoPositionalEncoding\n",
    "\n",
    "# Transformer Config: (emsize, nhead, nhid, nlayers, dropout)\n",
    "transformer_config = (args['emsize'], args['nhead'], args['nhid'], args['nlayers'], args['dropout'])\n",
    "\n",
    "# 6. Run Training\n",
    "print(f\"Starting training on {args['device']}...\")\n",
    "final_loss, positional_losses, model = train.train(\n",
    "    prior_dataloader=prior,\n",
    "    criterion=criterion, # Passing the wrapper\n",
    "    encoder_generator=encoder_generator,\n",
    "    transformer_configuration=transformer_config,\n",
    "    y_encoder_generator=encoder_generator, # Using same encoder type for y\n",
    "    pos_encoder_generator=pos_encoder_generator,\n",
    "    epochs=args['epochs'],\n",
    "    steps_per_epoch=args['steps_per_epoch'],\n",
    "    batch_size=args['batch_size'],\n",
    "    sequence_length=args['sequence_length'],\n",
    "    lr=args['lr'],\n",
    "    prior_hyperparameters=args['prior_hyperparameters'],\n",
    "    device=args['device'],\n",
    "    verbose=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a1f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure((15, 5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
